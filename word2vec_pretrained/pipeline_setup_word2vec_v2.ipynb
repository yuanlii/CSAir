{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "README: The scripts below used to build the basic pipeline of classification modeling. More to try include: <br>\n",
    " - embedding: try pretrained models\n",
    " - add: tf-idf processing\n",
    " - modeling: try other modeling methods except for naive bayes; hyperparameter tuning\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the whole dataset include 1623 reviews\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11 月 15 日 提前 预订 2018 年 11 月 27 日 长沙 飞往 沈阳 cz3...</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>航班 延误 登机口 升舱 活动 以原 航班 起飞时间 为准 办理 理解</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>重庆 乌鲁木齐 南航 航班 天气 原因 延误 和田 乘坐 天津 航班</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>沿途 停靠 理解 延误 小时</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>飞机 无故 延误 小时 脸</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>延误 五个 小时 算上 值机 时间 机场 八个 小时 早上 晚上 解释 解决方案 机长 人影...</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cz3842 航班 延误 投诉无门 十点 五十 起飞 下午 三点 弄 飞机 两个 小时 告知...</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>南航 航班 延误 发 短信 太 严谨 回复 改 航班 用户名 密码 我要 变更 航班 做 延...</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>行李 延误   重大损失</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>确认 航班 延误   订 票   显示 确认</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       review_tokens label\n",
       "0   11 月 15 日 提前 预订 2018 年 11 月 27 日 长沙 飞往 沈阳 cz3...    出发\n",
       "1               航班 延误 登机口 升舱 活动 以原 航班 起飞时间 为准 办理 理解     出发\n",
       "2                重庆 乌鲁木齐 南航 航班 天气 原因 延误 和田 乘坐 天津 航班     出发\n",
       "3                                    沿途 停靠 理解 延误 小时     出发\n",
       "4                                     飞机 无故 延误 小时 脸     出发\n",
       "5  延误 五个 小时 算上 值机 时间 机场 八个 小时 早上 晚上 解释 解决方案 机长 人影...    出发\n",
       "6  cz3842 航班 延误 投诉无门 十点 五十 起飞 下午 三点 弄 飞机 两个 小时 告知...    出发\n",
       "7  南航 航班 延误 发 短信 太 严谨 回复 改 航班 用户名 密码 我要 变更 航班 做 延...    出发\n",
       "8                                      行李 延误   重大损失     出发\n",
       "9                            确认 航班 延误   订 票   显示 确认     出发"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "combine dataset (multiple categories) into one single category;\n",
    "add a column called 'label'\n",
    "'''\n",
    "\n",
    "files= glob.glob('../output_data/*.txt')\n",
    "\n",
    "df_lst = []\n",
    "for f in files:\n",
    "    label = f.split('/')[-1][:2]\n",
    "    df = pd.read_csv(f,header=None)\n",
    "    df['label'] = label\n",
    "    df_lst.append(df)\n",
    "\n",
    "all_df = pd.concat(df_lst)\n",
    "print('the whole dataset include %d reviews'%len(all_df))\n",
    "all_df = all_df.rename(columns = {0:'review_tokens'})\n",
    "all_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'出发': 352, '到达': 147, '性能': 148, '售后': 166, '设计': 47, '计划': 38, '机上': 299, '预订': 218, '中转': 147, '行程': 61}\n"
     ]
    }
   ],
   "source": [
    "# get the data size for each label\n",
    "labels = all_df.label.unique().tolist()\n",
    "label_size = {}\n",
    "for label in labels:\n",
    "    label_size[label] = len(all_df[all_df.label == label])\n",
    "\n",
    "print(label_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11 月 15 日 提前 预订 2018 年 11 月 27 日 长沙 飞往 沈阳 cz3...</td>\n",
       "      <td>出发</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>航班 延误 登机口 升舱 活动 以原 航班 起飞时间 为准 办理 理解</td>\n",
       "      <td>出发</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>重庆 乌鲁木齐 南航 航班 天气 原因 延误 和田 乘坐 天津 航班</td>\n",
       "      <td>出发</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>沿途 停靠 理解 延误 小时</td>\n",
       "      <td>出发</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>飞机 无故 延误 小时 脸</td>\n",
       "      <td>出发</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       review_tokens label  label_encoded\n",
       "0   11 月 15 日 提前 预订 2018 年 11 月 27 日 长沙 飞往 沈阳 cz3...    出发              1\n",
       "1               航班 延误 登机口 升舱 活动 以原 航班 起飞时间 为准 办理 理解     出发              1\n",
       "2                重庆 乌鲁木齐 南航 航班 天气 原因 延误 和田 乘坐 天津 航班     出发              1\n",
       "3                                    沿途 停靠 理解 延误 小时     出发              1\n",
       "4                                     飞机 无故 延误 小时 脸     出发              1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode text label into numbers\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "targets = le.fit_transform(all_df.label)\n",
    "all_df['label_encoded'] = targets\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average review length: 51.01\n",
      "maximum review length: 648.00\n",
      "minimum review length: 2.00\n"
     ]
    }
   ],
   "source": [
    "# descriptive analysis: get the average length of user reviews \n",
    "lengths = []\n",
    "for i in range(len(all_df)):\n",
    "    length = len(all_df['review_tokens'].iloc[i])\n",
    "    lengths.append(length)\n",
    "\n",
    "print('average review length: '+ '{:.2f}'.format(np.average(lengths)))\n",
    "print('maximum review length: '+ '{:.2f}'.format(np.max(lengths)))\n",
    "print('minimum review length: '+ '{:.2f}'.format(np.min(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test split data\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train, test = train_test_split(all_df, test_size=0.33, random_state=42)\n",
    "# print('training data has %d examples' %len(train))\n",
    "# print('test data has %d examples' %len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100 # 每条新闻最大长度\n",
    "EMBEDDING_DIM = 200 # 词向量空间维度\n",
    "VALIDATION_SPLIT = 0.16 # 验证集比例\n",
    "TEST_SPLIT = 0.2 # 测试集比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5169 unique tokens.\n",
      "Shape of data tensor: (1623, 100)\n",
      "Shape of label tensor: (1623, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "all_texts = all_df['review_tokens']\n",
    "all_labels = all_df['label_encoded']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "sequences = tokenizer.texts_to_sequences(all_texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(all_labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train docs: 1038\n",
      "val docs: 260\n",
      "test docs: 325\n"
     ]
    }
   ],
   "source": [
    "# 再将处理后的新闻数据按 6.4：1.6：2 分为训练集，验证集，测试集\n",
    "p1 = int(len(data)*(1-VALIDATION_SPLIT-TEST_SPLIT))\n",
    "p2 = int(len(data)*(1-TEST_SPLIT))\n",
    "x_train = data[:p1]\n",
    "y_train = labels[:p1]\n",
    "x_val = data[p1:p2]\n",
    "y_val = labels[p1:p2]\n",
    "x_test = data[p2:]\n",
    "y_test = labels[p2:]\n",
    "print ('train docs: '+str(len(x_train)))\n",
    "print ('val docs: '+str(len(x_val)))\n",
    "print ('test docs: '+str(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 200)          1034000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 200)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 98, 250)           150250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 32, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               1600200   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 2,786,460\n",
      "Trainable params: 2,786,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1038 samples, validate on 260 samples\n",
      "Epoch 1/5\n",
      "1038/1038 [==============================] - 4s 4ms/step - loss: 0.7991 - acc: 0.7293 - val_loss: 4.7520 - val_acc: 0.2731\n",
      "Epoch 2/5\n",
      "1038/1038 [==============================] - 3s 3ms/step - loss: 0.7773 - acc: 0.7399 - val_loss: 4.8728 - val_acc: 0.3000\n",
      "Epoch 3/5\n",
      "1038/1038 [==============================] - 3s 3ms/step - loss: 0.7716 - acc: 0.7543 - val_loss: 4.9882 - val_acc: 0.2885\n",
      "Epoch 4/5\n",
      "1038/1038 [==============================] - 3s 3ms/step - loss: 0.7600 - acc: 0.7447 - val_loss: 5.0711 - val_acc: 0.2654\n",
      "Epoch 5/5\n",
      "1038/1038 [==============================] - 3s 3ms/step - loss: 0.7567 - acc: 0.7524 - val_loss: 5.1205 - val_acc: 0.2731\n",
      "325/325 [==============================] - 0s 809us/step\n",
      "[10.973212864215558, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='SGD',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs= 5, verbose=1, validation_data=(x_val, y_val))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-17 12:27:36,144 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "# train word2vec model using Chinese text resources (2005, by cityu, msr, pku)\n",
    "# reference word2vec documentation: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# code reference: https://github.com/kavgan/nlp-in-practice/blob/master/word2vec/Word2Vec.ipynb\n",
    "\n",
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, path_name):\n",
    "        self.path_name = path_name\n",
    " \n",
    "    def __iter__(self):\n",
    "        # notice: can only pass on text data\n",
    "        files = glob.glob(self.path_name)\n",
    "        for file in files:\n",
    "            with open(file) as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    yield line.split()\n",
    "\n",
    "# a memory-friendly iterator\n",
    "sentences = MySentences('../pretrained/icwb2-data/training/utf8_files/*.utf8') \n",
    "documents = list (sentences.__iter__())\n",
    "logging.info (\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-17 12:28:36,010 : INFO : collecting all words and their counts\n",
      "2019-03-17 12:28:36,014 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-17 12:28:36,116 : INFO : PROGRESS: at sentence #10000, processed 258856 words, keeping 26668 word types\n",
      "2019-03-17 12:28:36,190 : INFO : PROGRESS: at sentence #20000, processed 530513 words, keeping 39226 word types\n",
      "2019-03-17 12:28:36,306 : INFO : PROGRESS: at sentence #30000, processed 812550 words, keeping 49387 word types\n",
      "2019-03-17 12:28:36,386 : INFO : PROGRESS: at sentence #40000, processed 1089335 words, keeping 57562 word types\n",
      "2019-03-17 12:28:36,459 : INFO : PROGRESS: at sentence #50000, processed 1348940 words, keeping 65941 word types\n",
      "2019-03-17 12:28:36,539 : INFO : PROGRESS: at sentence #60000, processed 1629421 words, keeping 72909 word types\n",
      "2019-03-17 12:28:36,617 : INFO : PROGRESS: at sentence #70000, processed 1898087 words, keeping 78848 word types\n",
      "2019-03-17 12:28:36,694 : INFO : PROGRESS: at sentence #80000, processed 2170729 words, keeping 84214 word types\n",
      "2019-03-17 12:28:36,783 : INFO : PROGRESS: at sentence #90000, processed 2447978 words, keeping 97313 word types\n",
      "2019-03-17 12:28:36,863 : INFO : PROGRESS: at sentence #100000, processed 2712491 words, keeping 111568 word types\n",
      "2019-03-17 12:28:36,943 : INFO : PROGRESS: at sentence #110000, processed 2974020 words, keeping 120775 word types\n",
      "2019-03-17 12:28:37,031 : INFO : PROGRESS: at sentence #120000, processed 3233279 words, keeping 128121 word types\n",
      "2019-03-17 12:28:37,143 : INFO : PROGRESS: at sentence #130000, processed 3524892 words, keeping 138239 word types\n",
      "2019-03-17 12:28:37,253 : INFO : PROGRESS: at sentence #140000, processed 3824580 words, keeping 146859 word types\n",
      "2019-03-17 12:28:37,298 : INFO : PROGRESS: at sentence #150000, processed 3913359 words, keeping 150647 word types\n",
      "2019-03-17 12:28:37,333 : INFO : PROGRESS: at sentence #160000, processed 4001320 words, keeping 154149 word types\n",
      "2019-03-17 12:28:37,360 : INFO : PROGRESS: at sentence #170000, processed 4067231 words, keeping 155953 word types\n",
      "2019-03-17 12:28:37,392 : INFO : PROGRESS: at sentence #180000, processed 4141190 words, keeping 157068 word types\n",
      "2019-03-17 12:28:37,423 : INFO : PROGRESS: at sentence #190000, processed 4219303 words, keeping 158470 word types\n",
      "2019-03-17 12:28:37,454 : INFO : PROGRESS: at sentence #200000, processed 4293838 words, keeping 159641 word types\n",
      "2019-03-17 12:28:37,483 : INFO : PROGRESS: at sentence #210000, processed 4367475 words, keeping 160529 word types\n",
      "2019-03-17 12:28:37,523 : INFO : PROGRESS: at sentence #220000, processed 4458291 words, keeping 162228 word types\n",
      "2019-03-17 12:28:37,554 : INFO : PROGRESS: at sentence #230000, processed 4536721 words, keeping 164537 word types\n",
      "2019-03-17 12:28:37,594 : INFO : PROGRESS: at sentence #240000, processed 4622969 words, keeping 166563 word types\n",
      "2019-03-17 12:28:37,646 : INFO : PROGRESS: at sentence #250000, processed 4703520 words, keeping 169040 word types\n",
      "2019-03-17 12:28:37,692 : INFO : PROGRESS: at sentence #260000, processed 4776164 words, keeping 170433 word types\n",
      "2019-03-17 12:28:37,732 : INFO : PROGRESS: at sentence #270000, processed 4845753 words, keeping 171691 word types\n",
      "2019-03-17 12:28:37,763 : INFO : PROGRESS: at sentence #280000, processed 4908064 words, keeping 172581 word types\n",
      "2019-03-17 12:28:37,791 : INFO : PROGRESS: at sentence #290000, processed 4966109 words, keeping 173541 word types\n",
      "2019-03-17 12:28:37,867 : INFO : PROGRESS: at sentence #300000, processed 5032160 words, keeping 175059 word types\n",
      "2019-03-17 12:28:37,911 : INFO : PROGRESS: at sentence #310000, processed 5105287 words, keeping 177358 word types\n",
      "2019-03-17 12:28:37,948 : INFO : PROGRESS: at sentence #320000, processed 5182182 words, keeping 179607 word types\n",
      "2019-03-17 12:28:37,979 : INFO : PROGRESS: at sentence #330000, processed 5254831 words, keeping 181936 word types\n",
      "2019-03-17 12:28:38,013 : INFO : PROGRESS: at sentence #340000, processed 5329693 words, keeping 183757 word types\n",
      "2019-03-17 12:28:38,094 : INFO : PROGRESS: at sentence #350000, processed 5402910 words, keeping 185729 word types\n",
      "2019-03-17 12:28:38,168 : INFO : PROGRESS: at sentence #360000, processed 5477734 words, keeping 187588 word types\n",
      "2019-03-17 12:28:38,243 : INFO : PROGRESS: at sentence #370000, processed 5547966 words, keeping 189450 word types\n",
      "2019-03-17 12:28:38,295 : INFO : PROGRESS: at sentence #380000, processed 5621387 words, keeping 191143 word types\n",
      "2019-03-17 12:28:38,346 : INFO : PROGRESS: at sentence #390000, processed 5697502 words, keeping 193050 word types\n",
      "2019-03-17 12:28:38,462 : INFO : PROGRESS: at sentence #400000, processed 5770707 words, keeping 194617 word types\n",
      "2019-03-17 12:28:38,560 : INFO : PROGRESS: at sentence #410000, processed 5845405 words, keeping 196316 word types\n",
      "2019-03-17 12:28:38,631 : INFO : PROGRESS: at sentence #420000, processed 5933994 words, keeping 198292 word types\n",
      "2019-03-17 12:28:38,689 : INFO : PROGRESS: at sentence #430000, processed 6022132 words, keeping 199876 word types\n",
      "2019-03-17 12:28:38,760 : INFO : PROGRESS: at sentence #440000, processed 6107715 words, keeping 201209 word types\n",
      "2019-03-17 12:28:38,819 : INFO : PROGRESS: at sentence #450000, processed 6188988 words, keeping 202588 word types\n",
      "2019-03-17 12:28:38,874 : INFO : PROGRESS: at sentence #460000, processed 6268719 words, keeping 203864 word types\n",
      "2019-03-17 12:28:38,923 : INFO : PROGRESS: at sentence #470000, processed 6350657 words, keeping 205113 word types\n",
      "2019-03-17 12:28:38,961 : INFO : PROGRESS: at sentence #480000, processed 6432799 words, keeping 206486 word types\n",
      "2019-03-17 12:28:38,995 : INFO : PROGRESS: at sentence #490000, processed 6516012 words, keeping 207845 word types\n",
      "2019-03-17 12:28:39,041 : INFO : PROGRESS: at sentence #500000, processed 6597176 words, keeping 209598 word types\n",
      "2019-03-17 12:28:39,072 : INFO : PROGRESS: at sentence #510000, processed 6679884 words, keeping 210897 word types\n",
      "2019-03-17 12:28:39,126 : INFO : PROGRESS: at sentence #520000, processed 6765893 words, keeping 212113 word types\n",
      "2019-03-17 12:28:39,169 : INFO : PROGRESS: at sentence #530000, processed 6855427 words, keeping 213947 word types\n",
      "2019-03-17 12:28:39,214 : INFO : PROGRESS: at sentence #540000, processed 6934825 words, keeping 216679 word types\n",
      "2019-03-17 12:28:39,264 : INFO : PROGRESS: at sentence #550000, processed 7017983 words, keeping 219299 word types\n",
      "2019-03-17 12:28:39,313 : INFO : PROGRESS: at sentence #560000, processed 7100504 words, keeping 220908 word types\n",
      "2019-03-17 12:28:39,365 : INFO : PROGRESS: at sentence #570000, processed 7169770 words, keeping 221473 word types\n",
      "2019-03-17 12:28:39,416 : INFO : PROGRESS: at sentence #580000, processed 7236076 words, keeping 221708 word types\n",
      "2019-03-17 12:28:39,474 : INFO : PROGRESS: at sentence #590000, processed 7305117 words, keeping 222215 word types\n",
      "2019-03-17 12:28:39,531 : INFO : PROGRESS: at sentence #600000, processed 7375869 words, keeping 223465 word types\n",
      "2019-03-17 12:28:39,585 : INFO : PROGRESS: at sentence #610000, processed 7448234 words, keeping 224586 word types\n",
      "2019-03-17 12:28:39,645 : INFO : PROGRESS: at sentence #620000, processed 7518328 words, keeping 225452 word types\n",
      "2019-03-17 12:28:39,690 : INFO : PROGRESS: at sentence #630000, processed 7591296 words, keeping 226490 word types\n",
      "2019-03-17 12:28:39,741 : INFO : PROGRESS: at sentence #640000, processed 7681051 words, keeping 227499 word types\n",
      "2019-03-17 12:28:39,794 : INFO : PROGRESS: at sentence #650000, processed 7772397 words, keeping 228873 word types\n",
      "2019-03-17 12:28:39,842 : INFO : PROGRESS: at sentence #660000, processed 7857283 words, keeping 229817 word types\n",
      "2019-03-17 12:28:39,883 : INFO : PROGRESS: at sentence #670000, processed 7924414 words, keeping 230727 word types\n",
      "2019-03-17 12:28:39,917 : INFO : PROGRESS: at sentence #680000, processed 7985814 words, keeping 231185 word types\n",
      "2019-03-17 12:28:39,958 : INFO : PROGRESS: at sentence #690000, processed 8052505 words, keeping 231860 word types\n",
      "2019-03-17 12:28:40,010 : INFO : PROGRESS: at sentence #700000, processed 8121806 words, keeping 233065 word types\n",
      "2019-03-17 12:28:40,047 : INFO : PROGRESS: at sentence #710000, processed 8173057 words, keeping 233521 word types\n",
      "2019-03-17 12:28:40,083 : INFO : PROGRESS: at sentence #720000, processed 8230876 words, keeping 233819 word types\n",
      "2019-03-17 12:28:40,134 : INFO : PROGRESS: at sentence #730000, processed 8299622 words, keeping 234738 word types\n",
      "2019-03-17 12:28:40,195 : INFO : PROGRESS: at sentence #740000, processed 8388590 words, keeping 236380 word types\n",
      "2019-03-17 12:28:40,273 : INFO : PROGRESS: at sentence #750000, processed 8477651 words, keeping 237849 word types\n",
      "2019-03-17 12:28:40,360 : INFO : PROGRESS: at sentence #760000, processed 8566404 words, keeping 239671 word types\n",
      "2019-03-17 12:28:40,428 : INFO : PROGRESS: at sentence #770000, processed 8644339 words, keeping 241388 word types\n",
      "2019-03-17 12:28:40,485 : INFO : PROGRESS: at sentence #780000, processed 8725948 words, keeping 242218 word types\n",
      "2019-03-17 12:28:40,532 : INFO : PROGRESS: at sentence #790000, processed 8805858 words, keeping 242793 word types\n",
      "2019-03-17 12:28:40,568 : INFO : PROGRESS: at sentence #800000, processed 8882964 words, keeping 243586 word types\n",
      "2019-03-17 12:28:40,600 : INFO : PROGRESS: at sentence #810000, processed 8953369 words, keeping 243947 word types\n",
      "2019-03-17 12:28:40,624 : INFO : PROGRESS: at sentence #820000, processed 9019649 words, keeping 244157 word types\n",
      "2019-03-17 12:28:40,664 : INFO : PROGRESS: at sentence #830000, processed 9099284 words, keeping 245560 word types\n",
      "2019-03-17 12:28:40,710 : INFO : PROGRESS: at sentence #840000, processed 9191615 words, keeping 246916 word types\n",
      "2019-03-17 12:28:40,762 : INFO : PROGRESS: at sentence #850000, processed 9334541 words, keeping 249688 word types\n",
      "2019-03-17 12:28:41,000 : INFO : PROGRESS: at sentence #860000, processed 9932693 words, keeping 262667 word types\n",
      "2019-03-17 12:28:41,135 : INFO : collected 271774 word types from a corpus of 10383549 raw words and 867952 sentences\n",
      "2019-03-17 12:28:41,137 : INFO : Loading a fresh vocabulary\n",
      "2019-03-17 12:28:41,519 : INFO : min_count=2 retains 144207 unique words (53% of original 271774, drops 127567)\n",
      "2019-03-17 12:28:41,520 : INFO : min_count=2 leaves 10255982 word corpus (98% of original 10383549, drops 127567)\n",
      "2019-03-17 12:28:42,080 : INFO : deleting the raw counts dictionary of 271774 items\n",
      "2019-03-17 12:28:42,088 : INFO : sample=0.001 downsamples 25 most-common words\n",
      "2019-03-17 12:28:42,089 : INFO : downsampling leaves estimated 8320716 word corpus (81.1% of prior 10255982)\n",
      "2019-03-17 12:28:42,675 : INFO : estimated required memory for 144207 words and 100 dimensions: 187469100 bytes\n",
      "2019-03-17 12:28:42,676 : INFO : resetting layer weights\n",
      "2019-03-17 12:28:44,882 : INFO : training model with 2 workers on 144207 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-03-17 12:28:45,899 : INFO : EPOCH 1 - PROGRESS: at 3.11% examples, 580911 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:28:46,908 : INFO : EPOCH 1 - PROGRESS: at 6.32% examples, 592307 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:28:47,921 : INFO : EPOCH 1 - PROGRESS: at 9.27% examples, 580015 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:28:48,934 : INFO : EPOCH 1 - PROGRESS: at 12.63% examples, 592901 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:28:49,948 : INFO : EPOCH 1 - PROGRESS: at 15.69% examples, 595072 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:28:50,958 : INFO : EPOCH 1 - PROGRESS: at 26.15% examples, 599712 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:28:51,965 : INFO : EPOCH 1 - PROGRESS: at 38.14% examples, 598815 words/s, in_qsize 4, out_qsize 0\n",
      "2019-03-17 12:28:52,977 : INFO : EPOCH 1 - PROGRESS: at 48.36% examples, 590188 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:28:53,982 : INFO : EPOCH 1 - PROGRESS: at 58.78% examples, 590177 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:28:54,993 : INFO : EPOCH 1 - PROGRESS: at 70.33% examples, 591085 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:28:56,002 : INFO : EPOCH 1 - PROGRESS: at 83.15% examples, 593165 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:28:57,017 : INFO : EPOCH 1 - PROGRESS: at 94.66% examples, 595473 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:28:58,020 : INFO : EPOCH 1 - PROGRESS: at 98.70% examples, 593097 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:28:58,904 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 12:28:58,918 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 12:28:58,919 : INFO : EPOCH - 1 : training on 10383549 raw words (8321159 effective words) took 14.0s, 593139 effective words/s\n",
      "2019-03-17 12:28:59,930 : INFO : EPOCH 2 - PROGRESS: at 3.02% examples, 568240 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:00,932 : INFO : EPOCH 2 - PROGRESS: at 6.36% examples, 600217 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:01,937 : INFO : EPOCH 2 - PROGRESS: at 9.56% examples, 602890 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:02,940 : INFO : EPOCH 2 - PROGRESS: at 12.55% examples, 593577 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:03,950 : INFO : EPOCH 2 - PROGRESS: at 15.69% examples, 599305 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:04,953 : INFO : EPOCH 2 - PROGRESS: at 23.82% examples, 581791 words/s, in_qsize 4, out_qsize 0\n",
      "2019-03-17 12:29:05,960 : INFO : EPOCH 2 - PROGRESS: at 31.90% examples, 560350 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:06,960 : INFO : EPOCH 2 - PROGRESS: at 43.17% examples, 559113 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:07,977 : INFO : EPOCH 2 - PROGRESS: at 53.03% examples, 557383 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:08,980 : INFO : EPOCH 2 - PROGRESS: at 63.54% examples, 561630 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:09,983 : INFO : EPOCH 2 - PROGRESS: at 75.11% examples, 564699 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:10,998 : INFO : EPOCH 2 - PROGRESS: at 87.62% examples, 568254 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:12,004 : INFO : EPOCH 2 - PROGRESS: at 97.51% examples, 565913 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:13,008 : INFO : EPOCH 2 - PROGRESS: at 98.96% examples, 561750 words/s, in_qsize 4, out_qsize 0\n",
      "2019-03-17 12:29:13,726 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 12:29:13,735 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 12:29:13,736 : INFO : EPOCH - 2 : training on 10383549 raw words (8322165 effective words) took 14.8s, 561997 effective words/s\n",
      "2019-03-17 12:29:14,742 : INFO : EPOCH 3 - PROGRESS: at 3.11% examples, 585908 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:15,744 : INFO : EPOCH 3 - PROGRESS: at 6.28% examples, 592908 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:16,796 : INFO : EPOCH 3 - PROGRESS: at 8.79% examples, 544008 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:17,812 : INFO : EPOCH 3 - PROGRESS: at 11.40% examples, 532507 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:18,812 : INFO : EPOCH 3 - PROGRESS: at 14.72% examples, 551968 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:19,816 : INFO : EPOCH 3 - PROGRESS: at 21.63% examples, 559169 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:20,822 : INFO : EPOCH 3 - PROGRESS: at 33.92% examples, 567489 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:21,825 : INFO : EPOCH 3 - PROGRESS: at 45.65% examples, 571620 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:22,832 : INFO : EPOCH 3 - PROGRESS: at 55.29% examples, 568662 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:23,835 : INFO : EPOCH 3 - PROGRESS: at 66.08% examples, 571574 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:24,841 : INFO : EPOCH 3 - PROGRESS: at 77.69% examples, 573560 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:25,854 : INFO : EPOCH 3 - PROGRESS: at 90.09% examples, 577422 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:26,857 : INFO : EPOCH 3 - PROGRESS: at 98.28% examples, 580721 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:27,870 : INFO : EPOCH 3 - PROGRESS: at 99.65% examples, 579672 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:28,069 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 12:29:28,085 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 12:29:28,085 : INFO : EPOCH - 3 : training on 10383549 raw words (8319964 effective words) took 14.3s, 580029 effective words/s\n",
      "2019-03-17 12:29:29,104 : INFO : EPOCH 4 - PROGRESS: at 3.28% examples, 615598 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:30,105 : INFO : EPOCH 4 - PROGRESS: at 6.58% examples, 619852 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:31,108 : INFO : EPOCH 4 - PROGRESS: at 9.27% examples, 584263 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:32,131 : INFO : EPOCH 4 - PROGRESS: at 11.88% examples, 559879 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:33,148 : INFO : EPOCH 4 - PROGRESS: at 14.72% examples, 554059 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:34,156 : INFO : EPOCH 4 - PROGRESS: at 21.63% examples, 560652 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:35,164 : INFO : EPOCH 4 - PROGRESS: at 34.08% examples, 569682 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:36,168 : INFO : EPOCH 4 - PROGRESS: at 44.85% examples, 567520 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:37,177 : INFO : EPOCH 4 - PROGRESS: at 55.56% examples, 571079 words/s, in_qsize 4, out_qsize 0\n",
      "2019-03-17 12:29:38,181 : INFO : EPOCH 4 - PROGRESS: at 65.35% examples, 569139 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:39,198 : INFO : EPOCH 4 - PROGRESS: at 76.09% examples, 567262 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:40,213 : INFO : EPOCH 4 - PROGRESS: at 88.23% examples, 569377 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:41,225 : INFO : EPOCH 4 - PROGRESS: at 98.05% examples, 572236 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:42,250 : INFO : EPOCH 4 - PROGRESS: at 99.47% examples, 573146 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:42,637 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 12:29:42,651 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 12:29:42,652 : INFO : EPOCH - 4 : training on 10383549 raw words (8319646 effective words) took 14.6s, 571701 effective words/s\n",
      "2019-03-17 12:29:43,666 : INFO : EPOCH 5 - PROGRESS: at 3.28% examples, 613474 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:44,678 : INFO : EPOCH 5 - PROGRESS: at 6.00% examples, 559495 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:45,695 : INFO : EPOCH 5 - PROGRESS: at 8.33% examples, 517804 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:46,712 : INFO : EPOCH 5 - PROGRESS: at 10.14% examples, 475224 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:47,717 : INFO : EPOCH 5 - PROGRESS: at 12.22% examples, 459373 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:48,735 : INFO : EPOCH 5 - PROGRESS: at 14.68% examples, 459140 words/s, in_qsize 4, out_qsize 0\n",
      "2019-03-17 12:29:49,738 : INFO : EPOCH 5 - PROGRESS: at 18.78% examples, 460150 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:50,748 : INFO : EPOCH 5 - PROGRESS: at 28.92% examples, 469696 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:51,764 : INFO : EPOCH 5 - PROGRESS: at 37.98% examples, 464423 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:52,764 : INFO : EPOCH 5 - PROGRESS: at 46.42% examples, 461305 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:53,772 : INFO : EPOCH 5 - PROGRESS: at 54.85% examples, 463083 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:54,775 : INFO : EPOCH 5 - PROGRESS: at 64.23% examples, 469252 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:55,789 : INFO : EPOCH 5 - PROGRESS: at 75.81% examples, 478349 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:56,790 : INFO : EPOCH 5 - PROGRESS: at 87.24% examples, 483607 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:57,800 : INFO : EPOCH 5 - PROGRESS: at 96.45% examples, 483705 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:58,815 : INFO : EPOCH 5 - PROGRESS: at 99.09% examples, 492525 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:29:59,412 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 12:29:59,424 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 12:29:59,425 : INFO : EPOCH - 5 : training on 10383549 raw words (8321067 effective words) took 16.8s, 496296 effective words/s\n",
      "2019-03-17 12:29:59,426 : INFO : training on a 51917745 raw words (41604001 effective words) took 74.5s, 558140 effective words/s\n",
      "2019-03-17 12:29:59,427 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-03-17 12:29:59,428 : INFO : training model with 2 workers on 144207 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-03-17 12:30:00,452 : INFO : EPOCH 1 - PROGRESS: at 3.32% examples, 618176 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:01,452 : INFO : EPOCH 1 - PROGRESS: at 6.66% examples, 625635 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:02,454 : INFO : EPOCH 1 - PROGRESS: at 9.68% examples, 609506 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:03,458 : INFO : EPOCH 1 - PROGRESS: at 12.94% examples, 610746 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:04,467 : INFO : EPOCH 1 - PROGRESS: at 16.00% examples, 611340 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:05,484 : INFO : EPOCH 1 - PROGRESS: at 27.23% examples, 612370 words/s, in_qsize 4, out_qsize 0\n",
      "2019-03-17 12:30:06,485 : INFO : EPOCH 1 - PROGRESS: at 36.41% examples, 588420 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:07,493 : INFO : EPOCH 1 - PROGRESS: at 46.27% examples, 577716 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:08,498 : INFO : EPOCH 1 - PROGRESS: at 56.67% examples, 579270 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:09,500 : INFO : EPOCH 1 - PROGRESS: at 67.55% examples, 580385 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:10,504 : INFO : EPOCH 1 - PROGRESS: at 79.47% examples, 582400 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:11,505 : INFO : EPOCH 1 - PROGRESS: at 88.23% examples, 571687 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:12,529 : INFO : EPOCH 1 - PROGRESS: at 97.85% examples, 567665 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:13,545 : INFO : EPOCH 1 - PROGRESS: at 99.34% examples, 570996 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:13,982 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 12:30:13,982 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 12:30:13,983 : INFO : EPOCH - 1 : training on 10383549 raw words (8320516 effective words) took 14.5s, 572060 effective words/s\n",
      "2019-03-17 12:30:14,998 : INFO : EPOCH 2 - PROGRESS: at 3.24% examples, 608320 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:16,000 : INFO : EPOCH 2 - PROGRESS: at 6.58% examples, 619811 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:17,012 : INFO : EPOCH 2 - PROGRESS: at 9.76% examples, 614341 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:18,021 : INFO : EPOCH 2 - PROGRESS: at 12.76% examples, 601481 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:19,028 : INFO : EPOCH 2 - PROGRESS: at 15.89% examples, 605912 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:20,029 : INFO : EPOCH 2 - PROGRESS: at 26.58% examples, 606989 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:21,045 : INFO : EPOCH 2 - PROGRESS: at 38.91% examples, 606556 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:22,053 : INFO : EPOCH 2 - PROGRESS: at 50.22% examples, 606306 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:23,055 : INFO : EPOCH 2 - PROGRESS: at 59.86% examples, 599286 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:24,069 : INFO : EPOCH 2 - PROGRESS: at 71.47% examples, 598076 words/s, in_qsize 4, out_qsize 0\n",
      "2019-03-17 12:30:25,077 : INFO : EPOCH 2 - PROGRESS: at 84.38% examples, 600330 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:26,079 : INFO : EPOCH 2 - PROGRESS: at 95.64% examples, 602054 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:27,092 : INFO : EPOCH 2 - PROGRESS: at 98.96% examples, 603798 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:27,840 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 12:30:27,845 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 12:30:27,846 : INFO : EPOCH - 2 : training on 10383549 raw words (8320349 effective words) took 13.9s, 600677 effective words/s\n",
      "2019-03-17 12:30:28,865 : INFO : EPOCH 3 - PROGRESS: at 3.36% examples, 626382 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:29,867 : INFO : EPOCH 3 - PROGRESS: at 6.61% examples, 621427 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:30,874 : INFO : EPOCH 3 - PROGRESS: at 9.93% examples, 624255 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:31,881 : INFO : EPOCH 3 - PROGRESS: at 13.29% examples, 625707 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:32,891 : INFO : EPOCH 3 - PROGRESS: at 16.04% examples, 611744 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:33,896 : INFO : EPOCH 3 - PROGRESS: at 27.23% examples, 612610 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:34,906 : INFO : EPOCH 3 - PROGRESS: at 39.21% examples, 608460 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:35,923 : INFO : EPOCH 3 - PROGRESS: at 50.63% examples, 608322 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:36,942 : INFO : EPOCH 3 - PROGRESS: at 61.44% examples, 608000 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:37,946 : INFO : EPOCH 3 - PROGRESS: at 72.42% examples, 601556 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:38,959 : INFO : EPOCH 3 - PROGRESS: at 85.16% examples, 603374 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:39,970 : INFO : EPOCH 3 - PROGRESS: at 96.18% examples, 602994 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:40,979 : INFO : EPOCH 3 - PROGRESS: at 99.04% examples, 604883 words/s, in_qsize 3, out_qsize 0\n",
      "2019-03-17 12:30:41,569 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-17 12:30:41,580 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-17 12:30:41,581 : INFO : EPOCH - 3 : training on 10383549 raw words (8320106 effective words) took 13.7s, 606035 effective words/s\n",
      "2019-03-17 12:30:41,581 : INFO : training on a 31150647 raw words (24960971 effective words) took 42.2s, 592183 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24960971, 31150647)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec (documents, window=5, min_count=2, workers=2)\n",
    "model.train(documents,total_examples=len(documents),epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-17 13:54:27,040 : INFO : saving Word2Vec object under word2vec.model, separately None\n",
      "2019-03-17 13:54:27,043 : INFO : storing np array 'vectors' to word2vec.model.wv.vectors.npy\n",
      "2019-03-17 13:54:27,444 : INFO : not storing attribute vectors_norm\n",
      "2019-03-17 13:54:27,447 : INFO : storing np array 'syn1neg' to word2vec.model.trainables.syn1neg.npy\n",
      "2019-03-17 13:54:27,754 : INFO : not storing attribute cum_table\n",
      "2019-03-17 13:54:28,912 : INFO : saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('紓解', 0.6620273590087891),\n",
       " ('改進', 0.6593138575553894),\n",
       " ('缓解', 0.6353970766067505),\n",
       " ('提昇', 0.6322538256645203),\n",
       " ('整頓', 0.6136868000030518),\n",
       " ('降低', 0.6037946939468384),\n",
       " ('用水', 0.601423442363739),\n",
       " ('確保', 0.5922541618347168),\n",
       " ('完善', 0.5906819105148315),\n",
       " ('改进', 0.5863096714019775)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at some example\n",
    "w1 = \"改善\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2764854474595333"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"使用\",w2=\"服务\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'行李'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"服务\",\"行李\",\"飞行\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0809084 ,  0.9275192 , -0.29201597, -0.81767106, -1.6600899 ,\n",
       "        2.2001536 ,  1.0700545 , -0.8595912 ,  2.9435027 ,  0.8626768 ,\n",
       "       -0.72343904, -0.961094  ,  0.6855952 , -1.0474694 , -3.453282  ,\n",
       "        1.5999223 ,  2.5249639 ,  3.4341424 ,  1.3286707 ,  2.2232103 ,\n",
       "       -1.7445086 , -2.2384393 ,  0.28517032,  0.81533754, -0.12209349,\n",
       "        0.50130093,  1.8650556 ,  2.2093637 ,  1.5667093 ,  1.1401491 ,\n",
       "        0.6444202 ,  1.30712   , -1.1403345 , -1.706028  ,  1.5846382 ,\n",
       "        0.9832784 , -2.7226467 , -1.2501054 , -1.4398317 ,  0.2591129 ,\n",
       "       -2.0718944 ,  0.7041562 , -1.1992522 , -0.05537521,  0.36487073,\n",
       "       -2.2382112 , -0.14641441,  0.6495357 , -0.31467563, -1.811874  ,\n",
       "        1.3798348 ,  0.13909039, -2.669851  , -0.49505627, -0.38020018,\n",
       "        1.3446413 , -0.05372661,  0.21918966, -0.61543125, -2.154251  ,\n",
       "       -0.36112723,  1.1066729 ,  1.2332584 ,  0.9838205 ,  1.59936   ,\n",
       "       -1.3186089 , -1.2351164 , -0.7976479 , -1.2930409 ,  1.1676357 ,\n",
       "       -1.3900467 , -1.714478  , -0.8106513 , -0.8457903 ,  2.3094997 ,\n",
       "       -0.9465445 ,  1.1369025 ,  0.47380507,  0.07282797, -1.5325445 ,\n",
       "       -5.488603  , -2.8957708 ,  0.35477215, -0.40527308, -0.38271463,\n",
       "       -0.5986526 , -1.1219862 , -0.7095157 ,  1.771947  , -2.8889432 ,\n",
       "       -1.8684359 , -0.18965147,  0.23588194, -2.8529806 , -2.344938  ,\n",
       "        4.562183  , -0.7400024 ,  0.4325431 , -1.1283097 ,  0.17938219],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get word vector\n",
    "model[\"服务\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33687514\n"
     ]
    }
   ],
   "source": [
    "a = np.array(model['服务'])\n",
    "b = np.array(model['改善'])\n",
    "\n",
    "# compute cosine similarity between two words\n",
    "similarity = a.dot(b) / (np.sqrt(np.sum(np.power(a, 2))) * np.sqrt(np.sum(np.power(b, 2))))\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100 # 每条新闻最大长度\n",
    "EMBEDDING_DIM = 200 # 词向量空间维度\n",
    "VALIDATION_SPLIT = 0.16 # 验证集比例\n",
    "TEST_SPLIT = 0.2 # 测试集比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5169 unique tokens.\n",
      "Shape of data tensor: (1623, 100)\n",
      "Shape of label tensor: (1623, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "sequences = tokenizer.texts_to_sequences(all_texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(all_labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train docs: 1038\n",
      "val docs: 260\n",
      "test docs: 325\n"
     ]
    }
   ],
   "source": [
    "p1 = int(len(data)*(1-VALIDATION_SPLIT-TEST_SPLIT))\n",
    "p2 = int(len(data)*(1-TEST_SPLIT))\n",
    "x_train = data[:p1]\n",
    "y_train = labels[:p1]\n",
    "x_val = data[p1:p2]\n",
    "y_val = labels[p1:p2]\n",
    "x_test = data[p2:]\n",
    "y_test = labels[p2:]\n",
    "print ('train docs: '+str(len(x_train)))\n",
    "print ('val docs: '+str(len(x_val)))\n",
    "print ('test docs: '+str(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 200)          1034000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 200)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 98, 250)           150250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 32, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               1600200   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 2,786,460\n",
      "Trainable params: 2,786,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1038/1038 [==============================] - 4s 4ms/step - loss: 0.7944 - acc: 0.7380\n",
      "Epoch 2/10\n",
      "1038/1038 [==============================] - 4s 4ms/step - loss: 0.6408 - acc: 0.7678\n",
      "Epoch 3/10\n",
      "1038/1038 [==============================] - 4s 4ms/step - loss: 0.5904 - acc: 0.7755\n",
      "Epoch 4/10\n",
      "1038/1038 [==============================] - 4s 4ms/step - loss: 0.5199 - acc: 0.7861\n",
      "Epoch 5/10\n",
      "1038/1038 [==============================] - 4s 4ms/step - loss: 0.4752 - acc: 0.8073\n",
      "Epoch 6/10\n",
      "1038/1038 [==============================] - 4s 4ms/step - loss: 0.4628 - acc: 0.7948\n",
      "Epoch 7/10\n",
      "1038/1038 [==============================] - 4s 4ms/step - loss: 0.4451 - acc: 0.8092\n",
      "Epoch 8/10\n",
      "1038/1038 [==============================] - 4s 4ms/step - loss: 0.4361 - acc: 0.8064\n",
      "Epoch 9/10\n",
      "1038/1038 [==============================] - 4s 4ms/step - loss: 0.4298 - acc: 0.8092\n",
      "Epoch 10/10\n",
      "1038/1038 [==============================] - 4s 4ms/step - loss: 0.4136 - acc: 0.8160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a38f76e10>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model fit training data\n",
    "model.fit(x_train,y_train,epochs = 10,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260/260 [==============================] - 1s 3ms/step\n",
      "val score: 7.592899425213154\n",
      "val accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "score, acc = model.evaluate(x_val, y_val,verbose=1)\n",
    "print('val score:', score)\n",
    "print('val accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 基于预训练的 word2vec 的 CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 100 # 词向量空间维度\n",
    "VALIDATION_SPLIT = 0.16 # 验证集比例\n",
    "TEST_SPLIT = 0.2 # 测试集比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5170, 100)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "import gensim\n",
    "\n",
    "def unicode(unicode_or_str):\n",
    "    '''convert between python2 and python3'''\n",
    "    if isinstance(unicode_or_str, str):\n",
    "        text = unicode_or_str\n",
    "        decoded = False\n",
    "    else:\n",
    "        text = unicode_or_str.decode(encoding)\n",
    "        decoded = True\n",
    "    return text\n",
    "\n",
    "# load pre-trained word2vev model\n",
    "w2v_model = gensim.models.Word2Vec.load(\"word2vec.model\")\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items(): \n",
    "    if unicode(word) in w2v_model:\n",
    "        embedding_matrix[i] = np.asarray(w2v_model[unicode(word)],\n",
    "                                         dtype='float32')\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 100)          517000    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 98, 250)           75250     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 32, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 100)               800100    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 1,393,360\n",
      "Trainable params: 876,360\n",
      "Non-trainable params: 517,000\n",
      "_________________________________________________________________\n",
      "Train on 1038 samples, validate on 260 samples\n",
      "Epoch 1/5\n",
      "1038/1038 [==============================] - 2s 2ms/step - loss: 1.9941 - acc: 0.2871 - val_loss: 3.8514 - val_acc: 0.0885\n",
      "Epoch 2/5\n",
      "1038/1038 [==============================] - 2s 1ms/step - loss: 1.5816 - acc: 0.4056 - val_loss: 4.2808 - val_acc: 0.0654\n",
      "Epoch 3/5\n",
      "1038/1038 [==============================] - 2s 1ms/step - loss: 1.2991 - acc: 0.4990 - val_loss: 3.8138 - val_acc: 0.4423\n",
      "Epoch 4/5\n",
      "1038/1038 [==============================] - 1s 1ms/step - loss: 1.1469 - acc: 0.5780 - val_loss: 5.3170 - val_acc: 0.0615\n",
      "Epoch 5/5\n",
      "1038/1038 [==============================] - 2s 2ms/step - loss: 1.0369 - acc: 0.6079 - val_loss: 6.1754 - val_acc: 0.0538\n",
      "325/325 [==============================] - 0s 530us/step\n",
      "[11.786586767343374, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "model.summary()\n",
    "# plot_model(model, to_file='model.png',show_shapes=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5)\n",
    "model.save('word_vector_cnn.h5')\n",
    "print (model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
