{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readme: Tasks include:\n",
    "* use fasttext for word embedding\n",
    "    * TODO: open pretrained word vectors\n",
    "    * TODO: build word embedding matrix\n",
    "    * TODO: modeling\n",
    "* make predictions based on probability by modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "os.chdir('/Users/liyuan/desktop/CSAir/codes')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "import tensorflow as tf\n",
    "from gensim.models import FastText\n",
    "\n",
    "from prepare_data import PrepareData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data has 1038 examples\n",
      "test data has 512 examples\n"
     ]
    }
   ],
   "source": [
    "# load data and split data\n",
    "data_p = PrepareData()\n",
    "labeled_data = data_p.load_data('../res/labeled_data_with_without_tk.csv')\n",
    "train, test = data_p.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chekcking\n",
    "labeled_data.head()\n",
    "int in labeled_data.review_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "def load_pretrained_vectors(file_path):\n",
    "    # TODO: need to exclude the first vector\n",
    "    embeddings_index = {}\n",
    "    f = open(file_path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 635922 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load pretrained 百度百科 pretrained word2vec vectors\n",
    "embeddings_index = load_pretrained_vectors('../Source_Data/sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using genism to handle word vectors, instead of load it directly\n",
    "# load dict file and get binary model\n",
    "# import gensim\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# # Baidu Baidu pretrained vectors\n",
    "# dictFileName = './Source_Data/sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5' \n",
    "# embedding_dict = gensim.models.KeyedVectors.load_word2vec_format(dictFileName, binary=False) \n",
    "# embedding_dict.save_word2vec_format(dictFileName+\".bin\", binary=True) \n",
    "# embedding_dict = gensim.models.KeyedVectors.load_word2vec_format(dictFileName+\".bin\", binary=True)\n",
    "# embedding_dict.most_similar('飞机')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText(object):\n",
    "    def __init__(self):\n",
    "        self.embeddings_index = {}\n",
    "        self.MAX_SEQUENCE_LENGTH = 1000\n",
    "        self.MAX_NUM_WORDS = 20000\n",
    "        self.EMBEDDING_DIM = 100\n",
    "        self.VALIDATION_SPLIT = 0.2\n",
    "        self.all_labeled_data = pd.DataFrame()\n",
    "        self.labels_index = {}\n",
    "        self.word_index  = {}\n",
    "        self.texts = np.array([])\n",
    "        self.labels = np.array([])\n",
    "        self.data = np.array([])\n",
    "        \n",
    "    def load_pretrained_vectors(self, file_path):\n",
    "        f = open(file_path)\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            self.embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "        print('Found %s word vectors.' % len(self.embeddings_index))\n",
    "        return self.embeddings_index     \n",
    "    \n",
    "    def load_source_data(self,data_file_path):\n",
    "        self.all_labeled_data = pd.read_csv(data_file_path)\n",
    "        return self.all_labeled_data\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        self.texts = self.all_labeled_data.review_tokens.values\n",
    "        self.labels = self.all_labeled_data.label_encoded.values\n",
    "        \n",
    "        # get a dictionary that map each original label to its encoded label, e.g., {'中转': 0,...}\n",
    "        for label in self.all_labeled_data.label.unique().tolist():\n",
    "            self.labels_index[label] = self.all_labeled_data[self.all_labeled_data['label'] == label]['label_encoded'].unique()\n",
    "        \n",
    "        tokenizer = Tokenizer(nb_words=self.MAX_NUM_WORDS)\n",
    "        tokenizer.fit_on_texts(self.texts)\n",
    "        sequences = tokenizer.texts_to_sequences(self.texts)\n",
    "        \n",
    "        self.word_index = tokenizer.word_index\n",
    "        print('Found %s unique tokens.' % len(self.word_index))\n",
    "        \n",
    "        self.data = pad_sequences(sequences, maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "        print('Shape of data tensor:', self.data.shape)\n",
    "        print('Shape of label tensor:', self.labels.shape)\n",
    "        \n",
    "        # Converts a class vector (integers) to binary class matrix\n",
    "        self.labels = to_categorical(np.asarray(self.labels))\n",
    "        print('Shape of data tensor:', self.data.shape)\n",
    "        print('Shape of label tensor:', self.labels.shape)\n",
    "        \n",
    "        # split the data into a training set and a validation set\n",
    "        indices = np.arange(self.data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        self.data = self.data[indices]\n",
    "        self.labels = self.labels[indices]\n",
    "        nb_validation_samples = int(self.VALIDATION_SPLIT * self.data.shape[0])\n",
    "        \n",
    "        self.X_train = self.data[:-nb_validation_samples]\n",
    "        self.y_train = self.labels[:-nb_validation_samples]\n",
    "        self.X_val = self.data[-nb_validation_samples:]\n",
    "        self.y_val = self.labels[-nb_validation_samples:]\n",
    "        return  self.X_train, self.y_train, self.X_val, self.y_val\n",
    "    \n",
    "    def get_embedding_matrix(self):\n",
    "        self.embedding_matrix = np.zeros((len(self.word_index) + 1, self.EMBEDDING_DIM))\n",
    "        for word, i in self.word_index.items():\n",
    "            embedding_vector = self.embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        return embedding_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 635922 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp/lib/python3.6/site-packages/keras_preprocessing/text.py:177: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-801e71d05065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_pretrained_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Source_Data/sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../res/labeled_data_with_without_tk.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-6189c31099cb>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self, data_file_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_NUM_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nlp/lib/python3.6/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    220\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                                             self.split)\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nlp/lib/python3.6/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \"\"\"\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "ft = FastText()\n",
    "embeddings_index = ft.load_pretrained_vectors('../Source_Data/sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5')\n",
    "X_train, y_train, X_val, y_val = ft.prepare_data('../res/labeled_data_with_without_tk.csv')\n",
    "embedding_matrix = ft.get_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
