{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "os.chdir('/Users/liyuan/desktop/CSAir')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "    def __init__(self):\n",
    "        self.embeddings_index = {}\n",
    "        self.MAX_SEQUENCE_LENGTH = 1000\n",
    "        self.MAX_NUM_WORDS = 20000\n",
    "        self.EMBEDDING_DIM = 100\n",
    "        self.VALIDATION_SPLIT = 0.2\n",
    "        self.all_labeled_data = pd.DataFrame()\n",
    "        self.labels_index = {}\n",
    "        self.word_index  = {}\n",
    "        self.texts = np.array([])\n",
    "        self.labels = np.array([])\n",
    "        self.data = np.array([])\n",
    "        self.X_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        self.X_val = np.array([])\n",
    "        self.y_val = np.array([])\n",
    "        self.embedding_matrix = np.array([])\n",
    "\n",
    "    def load_pretrained_vectors(self, file_path):\n",
    "        f = open(file_path)\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            self.embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "        print('Found %s word vectors.' % len(self.embeddings_index))\n",
    "        return self.embeddings_index \n",
    "\n",
    "    def prepare_data(self,data_file_path):\n",
    "        self.all_labeled_data = pd.read_csv(data_file_path)\n",
    "        self.texts = self.all_labeled_data.review_tokens.values\n",
    "        self.labels = self.all_labeled_data.label_encoded.values\n",
    "        \n",
    "        # get a dictionary that map each original label to its encoded label, e.g., {'中转': 0,...}\n",
    "        for label in self.all_labeled_data.label.unique().tolist():\n",
    "            self.labels_index[label] = self.all_labeled_data[self.all_labeled_data['label'] == label]['label_encoded'].unique()[0]\n",
    "\n",
    "        tokenizer = Tokenizer(nb_words=self.MAX_NUM_WORDS)\n",
    "        tokenizer.fit_on_texts(self.texts)\n",
    "        sequences = tokenizer.texts_to_sequences(self.texts)\n",
    "\n",
    "        self.word_index = tokenizer.word_index\n",
    "        print('Found %s unique tokens.' % len(self.word_index))\n",
    "\n",
    "        self.data = pad_sequences(sequences, maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "        print('Shape of data tensor:', self.data.shape)\n",
    "        print('Shape of label tensor:', self.labels.shape)\n",
    "        \n",
    "        # Converts a class vector (integers) to binary class matrix\n",
    "        self.labels = to_categorical(np.asarray(self.labels))\n",
    "        print('Shape of data tensor:', self.data.shape)\n",
    "        print('Shape of label tensor:', self.labels.shape)\n",
    "\n",
    "        # split the data into a training set and a validation set\n",
    "        indices = np.arange(self.data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        self.data = self.data[indices]\n",
    "        self.labels = self.labels[indices]\n",
    "        nb_validation_samples = int(self.VALIDATION_SPLIT * self.data.shape[0])\n",
    "\n",
    "        self.X_train = self.data[:-nb_validation_samples]\n",
    "        self.y_train = self.labels[:-nb_validation_samples]\n",
    "        self.X_val = self.data[-nb_validation_samples:]\n",
    "        self.y_val = self.labels[-nb_validation_samples:]\n",
    "        return  self.X_train, self.y_train, self.X_val, self.y_val\n",
    "    \n",
    "    # TODO:\n",
    "    def train_data(self):\n",
    "        # 据得到的字典生成上文所定义的词向量矩阵\n",
    "        self.embedding_matrix = np.zeros((len(self.word_index) + 1, self.EMBEDDING_DIM))\n",
    "        for word, i in self.word_index.items():\n",
    "            embedding_vector = self.embeddings_index.get(word)\n",
    "            \n",
    "        # 将这个词向量矩阵加载到Embedding层\n",
    "        embedding_layer = Embedding(len(self.word_index) + 1,\n",
    "                                    self.EMBEDDING_DIM,\n",
    "                                    weights=[self.embedding_matrix],\n",
    "                                    input_length=self.MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "        \n",
    "        # 使用一个小型的1D卷积解决分类问题\n",
    "        sequence_input = Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "        x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "        x = MaxPooling1D(5)(x)\n",
    "        x = Conv1D(128, 5, activation='relu')(x)\n",
    "        x = MaxPooling1D(5)(x)\n",
    "        x = Conv1D(128, 5, activation='relu')(x)\n",
    "        x = MaxPooling1D(35)(x)  # global max pooling\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        preds = Dense(len(self.labels_index), activation='softmax')(x)\n",
    "\n",
    "        model = Model(sequence_input, preds)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['acc'])\n",
    "        model.fit(self.X_train, self.y_train, validation_data=(self.X_val, self.y_val),\n",
    "                  nb_epoch=2, batch_size=128)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 635922 word vectors.\n",
      "Found 4684 unique tokens.\n",
      "Shape of data tensor: (1700, 1000)\n",
      "Shape of label tensor: (1700,)\n",
      "Shape of data tensor: (1700, 1000)\n",
      "Shape of label tensor: (1700, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:101: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1360 samples, validate on 340 samples\n",
      "Epoch 1/2\n",
      "1360/1360 [==============================] - 17s 12ms/step - loss: 2.2982 - acc: 0.1721 - val_loss: 2.2940 - val_acc: 0.1735\n",
      "Epoch 2/2\n",
      "1360/1360 [==============================] - 14s 10ms/step - loss: 2.2922 - acc: 0.1897 - val_loss: 2.2890 - val_acc: 0.2412\n"
     ]
    }
   ],
   "source": [
    "w2v = word2vec()\n",
    "embeddings_index = w2v.load_pretrained_vectors('./Source_Data/sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5')\n",
    "X_train, y_train, X_val, y_val = w2v.prepare_data('./res/all_labeled_data_v3.csv')\n",
    "model = w2v.train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09985197, 0.10336784, 0.0996991 , ..., 0.09717343, 0.0973144 ,\n",
       "        0.1021671 ],\n",
       "       [0.09985197, 0.10336784, 0.0996991 , ..., 0.09717343, 0.0973144 ,\n",
       "        0.1021671 ],\n",
       "       [0.09985197, 0.10336784, 0.0996991 , ..., 0.09717343, 0.0973144 ,\n",
       "        0.1021671 ],\n",
       "       ...,\n",
       "       [0.09985197, 0.10336784, 0.0996991 , ..., 0.09717343, 0.0973144 ,\n",
       "        0.1021671 ],\n",
       "       [0.09985197, 0.10336784, 0.0996991 , ..., 0.09717343, 0.0973144 ,\n",
       "        0.1021671 ],\n",
       "       [0.09985197, 0.10336784, 0.0996991 , ..., 0.09717343, 0.0973144 ,\n",
       "        0.1021671 ]], dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: convert predicted probability to class labels\n",
    "model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 据得到的字典生成上文所定义的词向量矩阵\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将这个词向量矩阵加载到Embedding层\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_13 to have shape (10,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-2de06138e6d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# the problem is here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/nlp/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nlp/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/nlp/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_13 to have shape (10,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# 使用一个小型的1D卷积解决分类问题\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# the problem is here \n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),nb_epoch=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 1000, 100)         468500    \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 714,526\n",
      "Trainable params: 246,026\n",
      "Non-trainable params: 468,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
