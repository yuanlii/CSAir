{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readme: \n",
    "* TODO: used smaller amount of text to train fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "os.chdir('/Users/liyuan/desktop/CSAir/codes')\n",
    "import fastText \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from semi_supervise import Semi_Supervise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1550 examples in labeled dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>生日优惠券还有领取时间限制的？！下次希望能标注一下领取时间限制</td>\n",
       "      <td>生日 优惠券 领取 时间 下次 希望 标注 领取 时间</td>\n",
       "      <td>计划</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>参加了11月1号的“11.11购50元抵用券”，至今未收到该优惠券。</td>\n",
       "      <td>参加 月 号 购 元 抵用券 未 收到 优惠券</td>\n",
       "      <td>计划</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>扑街，没有跟星巴克合作5折优惠，写上去干尼玛啊</td>\n",
       "      <td>扑街 星巴克 合作 折 优惠 写上去 干 尼玛</td>\n",
       "      <td>计划</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>请问广州飞墨尔本的航班超额行李要多少钱1kg.我收到你们发的信息是4折，但具体多少钱1kg不知道</td>\n",
       "      <td>请问 广州 飞 墨尔本 航班 超额 行李 钱 收到 发 信息 折 钱</td>\n",
       "      <td>计划</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>請問帶一個20吋行李箱加電腦後背包是否需加行李費</td>\n",
       "      <td>請 問帶 一個 吋 行李箱 加電腦 後 背包 需加 行李 費</td>\n",
       "      <td>计划</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             review  \\\n",
       "0                   生日优惠券还有领取时间限制的？！下次希望能标注一下领取时间限制   \n",
       "1                参加了11月1号的“11.11购50元抵用券”，至今未收到该优惠券。   \n",
       "2                           扑街，没有跟星巴克合作5折优惠，写上去干尼玛啊   \n",
       "3  请问广州飞墨尔本的航班超额行李要多少钱1kg.我收到你们发的信息是4折，但具体多少钱1kg不知道   \n",
       "4                          請問帶一個20吋行李箱加電腦後背包是否需加行李費   \n",
       "\n",
       "                        review_tokens label  label_encoded  \n",
       "0         生日 优惠券 领取 时间 下次 希望 标注 领取 时间    计划              7  \n",
       "1             参加 月 号 购 元 抵用券 未 收到 优惠券    计划              7  \n",
       "2             扑街 星巴克 合作 折 优惠 写上去 干 尼玛    计划              7  \n",
       "3  请问 广州 飞 墨尔本 航班 超额 行李 钱 收到 发 信息 折 钱    计划              7  \n",
       "4      請 問帶 一個 吋 行李箱 加電腦 後 背包 需加 行李 費    计划              7  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "ss = Semi_Supervise()\n",
    "labeled_data = ss.load_labeled_data('../res/labeled_data_with_without_tk.csv')\n",
    "# load unlabeled data\n",
    "unlabeled_data = ss.load_unlabeled_data_csv('../res/unlabeled_review_5000.csv')\n",
    "# concatenate labeled and unlabeled data\n",
    "data_concat = ss.concat_data()\n",
    "data_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output data to txt file => data would be feed into fasttext\n",
    "train_data = data_concat.review_tokens\n",
    "train_data.to_csv('../res/sampled_data_fasttext.txt', index = False)\n",
    "\n",
    "# Finished: locally train fasttext using labeled + unlabeled data (5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fasttext():\n",
    "    def __init__(self):\n",
    "        self.embeddings_index = {}\n",
    "        self.MAX_SEQUENCE_LENGTH = 1000\n",
    "        self.MAX_NUM_WORDS = 20000\n",
    "        self.EMBEDDING_DIM = 300\n",
    "        self.VALIDATION_SPLIT = 0.2\n",
    "        self.labels_index = {}\n",
    "        self.word_index  = {}\n",
    "    \n",
    "    def load_pretrained_model(self, model_path):\n",
    "        model_pretrained = fastText.load_model(model_path) \n",
    "        return model_pretrained\n",
    "    \n",
    "    def prepare_data(self,data_file_path):\n",
    "        self.all_labeled_data = pd.read_csv(data_file_path)\n",
    "        self.texts = self.all_labeled_data.review_tokens.astype('str').values\n",
    "        self.labels = self.all_labeled_data.label_encoded.values\n",
    "        \n",
    "        # get a dictionary that map each original label to its encoded label, e.g., {'中转': 0,...}\n",
    "        for label in self.all_labeled_data.label.unique().tolist():\n",
    "            self.labels_index[label] = self.all_labeled_data[self.all_labeled_data['label'] == label]['label_encoded'].unique()[0]\n",
    "\n",
    "        tokenizer = Tokenizer(nb_words=self.MAX_NUM_WORDS)\n",
    "        tokenizer.fit_on_texts(self.texts)\n",
    "        sequences = tokenizer.texts_to_sequences(self.texts)\n",
    "\n",
    "        self.word_index = tokenizer.word_index\n",
    "        print('Found %s unique tokens.' % len(self.word_index))\n",
    "\n",
    "        self.data = pad_sequences(sequences, maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "        print('Shape of data tensor:', self.data.shape)\n",
    "        print('Shape of label tensor:', self.labels.shape)\n",
    "        \n",
    "        # Converts a class vector (integers) to binary class matrix\n",
    "        self.labels = to_categorical(np.asarray(self.labels))\n",
    "        print('Shape of data tensor:', self.data.shape)\n",
    "        print('Shape of label tensor:', self.labels.shape)\n",
    "\n",
    "        # split the data into a training set and a validation set\n",
    "        self.indices = np.arange(self.data.shape[0])\n",
    "        np.random.shuffle(self.indices)\n",
    "        self.data = self.data[self.indices]\n",
    "        self.labels = self.labels[self.indices]\n",
    "        nb_validation_samples = int(self.VALIDATION_SPLIT * self.data.shape[0])\n",
    "\n",
    "        self.X_train = self.data[:-nb_validation_samples]\n",
    "        self.y_train = self.labels[:-nb_validation_samples]\n",
    "        self.X_val = self.data[-nb_validation_samples:]\n",
    "        self.y_val = self.labels[-nb_validation_samples:]\n",
    "        return  self.X_train, self.y_train, self.X_val, self.y_val\n",
    "    \n",
    "    def get_embedding_matrix(self):\n",
    "        # 据得到的字典生成上文所定义的词向量矩阵\n",
    "        embedding_matrix = np.zeros((len(self.word_index) + 1, self.EMBEDDING_DIM))\n",
    "        for word, i in self.word_index.items():\n",
    "            embedding_vector = self.embeddings_index.get(word)\n",
    "            # updated:\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        return embedding_matrix\n",
    "    \n",
    "    def setup_neural_net(self):\n",
    "        # get word embedding matrix\n",
    "        self.embedding_matrix = self.get_embedding_matrix()\n",
    "\n",
    "        # 将这个词向量矩阵加载到Embedding层\n",
    "        embedding_layer = Embedding(len(self.word_index) + 1,\n",
    "                                    self.EMBEDDING_DIM,\n",
    "                                    weights=[self.embedding_matrix],\n",
    "                                    input_length=self.MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "        \n",
    "        # 使用一个小型的1D卷积解决分类问题\n",
    "        sequence_input = Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "        x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "        x = MaxPooling1D(5)(x)\n",
    "        x = Conv1D(128, 5, activation='relu')(x)\n",
    "        x = MaxPooling1D(5)(x)\n",
    "        x = Conv1D(128, 5, activation='relu')(x)\n",
    "        x = MaxPooling1D(35)(x)  # global max pooling\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        preds = Dense(len(self.labels_index), activation='softmax')(x)\n",
    "        return sequence_input,preds\n",
    "    \n",
    "    \n",
    "    def train_data(self,X_train,y_train,X_val,y_val):\n",
    "        sequence_input,preds = self.setup_neural_net()\n",
    "        model = Model(sequence_input, preds)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['acc'])\n",
    "        # can change the number of epoch accordingly\n",
    "        # 7 generates the best performance\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                  nb_epoch=7, batch_size=128)  \n",
    "        \n",
    "        # evaluate model using model.evaluate()\n",
    "        scores = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "        \n",
    "        # get predicted class label\n",
    "        self.output = model.predict(X_val)\n",
    "        predicted_label_list = self.get_pred_label(self.output)\n",
    "        return predicted_label_list\n",
    "    \n",
    "    \n",
    "    def get_pred_label(self,output):\n",
    "        '''get predicted class label based on prediction output'''\n",
    "        predicted_label_list = []\n",
    "        for i in range(len(output)):\n",
    "            predicted_label = output[i].argmax(axis=-1)\n",
    "            predicted_label_list.append(predicted_label)        \n",
    "        return predicted_label_list\n",
    "    \n",
    "    \n",
    "    def incorporate_pred_label(self):\n",
    "        '''return prediction results back to df'''\n",
    "        # indices is a numpy array, need to convert to a list of indices before feed into df to get sub df \n",
    "        # recreate df based on the shuffled indices\n",
    "        indices = self.indices\n",
    "        all_labeled_data = self.all_labeled_data.iloc[list(self.indices)]\n",
    "        nb_validation_samples = int(self.VALIDATION_SPLIT * self.data.shape[0])\n",
    "        print(nb_validation_samples)\n",
    "        # need to get the indices of the validation data\n",
    "        train_val_bound = self.data.shape[0] - nb_validation_samples\n",
    "        # get validation dataset\n",
    "        val_df = all_labeled_data[train_val_bound:]\n",
    "        return val_df\n",
    "\n",
    "    def map_label(self,df,predicted_label_list):\n",
    "        '''map predicted labels to original class'''\n",
    "        # print(predicted_label_list[:10])\n",
    "        label_dct = self.labels_index\n",
    "        df['pred_label_encodes'] = predicted_label_list\n",
    "        # get reversed labels_index dictionary\n",
    "        reversed_label_dct = {}\n",
    "        for i in range(len(label_dct)):\n",
    "            reversed_label_dct[list(label_dct.values())[i]] = list(label_dct.keys())[i]\n",
    "\n",
    "        # map predicted labels\n",
    "        pred_label = [reversed_label_dct.get(label) for label in predicted_label_list]\n",
    "        df['pred_label'] = pred_label\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def evaluate_performance(self,val_df):\n",
    "        # evaluate performance\n",
    "        y_val_true = val_df.label.values\n",
    "        y_val_pred = val_df.pred_label.values\n",
    "        self.get_confusion_matrix(y_val_true,y_val_pred) \n",
    "        \n",
    "    \n",
    "    def get_confusion_matrix(self,y_test,y_pred):\n",
    "        '''get tp,tn,fp,fn for each class'''\n",
    "        cm = ConfusionMatrix(y_test, y_pred)\n",
    "        cm.print_stats()\n",
    "        \n",
    "        \n",
    "    def over_sampling(self):\n",
    "        '''modeling after over sampling'''\n",
    "        smote = SMOTE('minority')\n",
    "        X_train_sm, y_train_sm = smote.fit_sample(self.X_train,self.y_train)\n",
    "        print(X_train_sm.shape, y_train_sm.shape)\n",
    "        \n",
    "        # fit model based on new data set\n",
    "        predicted_label_list = self.train_data(X_train_sm,y_train_sm,X_val,y_val)\n",
    "        return predicted_label_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext()\n",
    "model = ft.load_pretrained_model('fasttext_train_data/model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimention of word vector: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.07510141,  0.34539855, -0.04847794, -0.00599669,  0.08026265,\n",
       "        0.17255239,  0.06303003,  0.16753493,  0.26827294,  0.24050438,\n",
       "        0.10935862, -0.05695121, -0.1612111 , -0.1254698 , -0.14411566,\n",
       "        0.1671324 , -0.09211409, -0.08527946,  0.07678948, -0.00543677,\n",
       "       -0.12030143,  0.13826734, -0.06652036, -0.22822875, -0.01887627,\n",
       "        0.00094851, -0.0210128 , -0.04052201, -0.07755461,  0.14083461,\n",
       "       -0.02262247,  0.05274943, -0.12807177, -0.10997214, -0.18952328,\n",
       "       -0.02350366,  0.2577952 , -0.2513076 , -0.15192115,  0.10377252,\n",
       "        0.14529383,  0.12857807, -0.13611504, -0.02255148,  0.3416287 ,\n",
       "       -0.12241936,  0.13015959,  0.11399453, -0.05882082,  0.11640163,\n",
       "        0.08050135, -0.07715302,  0.01431637, -0.13527611, -0.07535661,\n",
       "       -0.07706072, -0.02880922,  0.4216739 ,  0.09358777,  0.07875614,\n",
       "        0.17708524,  0.12715451,  0.03065475,  0.02836023,  0.10281598,\n",
       "       -0.06090513,  0.07658396,  0.17984973, -0.0974253 , -0.01285458,\n",
       "       -0.17587005,  0.0740779 , -0.01954268, -0.0369027 , -0.21346194,\n",
       "       -0.07796012,  0.15358968,  0.21927392, -0.01555008,  0.10152558,\n",
       "        0.14325912, -0.1450496 ,  0.10351609, -0.03536751, -0.09584553,\n",
       "       -0.1321252 ,  0.08872717,  0.16196701,  0.0489408 ,  0.03071725,\n",
       "        0.23057672,  0.12721165,  0.52103347, -0.13791195,  0.02442563,\n",
       "       -0.44769627,  0.02728627,  0.00585644,  0.04959832,  0.03868417],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get more model methods\n",
    "print('dimention of word vector:',model.get_dimension())\n",
    "# load word vector\n",
    "model.get_word_vector('航班').astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4747 unique tokens.\n",
      "Shape of data tensor: (1551, 1000)\n",
      "Shape of label tensor: (1551,)\n",
      "Shape of data tensor: (1551, 1000)\n",
      "Shape of label tensor: (1551, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val = ft.prepare_data('../res/labeled_data_with_without_tk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'fasttext' object has no attribute 'setup_neural_net'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-7c2583056463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_label_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-6472bf55f6ae>\u001b[0m in \u001b[0;36mtrain_data\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0msequence_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_neural_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         model.compile(loss='categorical_crossentropy',\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'fasttext' object has no attribute 'setup_neural_net'"
     ]
    }
   ],
   "source": [
    "predicted_label_list = ft.train_data(X_train,y_train,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check label index\n",
    "print('label encoding dictionary:', ft.labels_index)\n",
    "\n",
    "val_df = ft.incorporate_pred_label()\n",
    "val_df = ft.map_label(val_df,predicted_label_list)\n",
    "val_df.head()\n",
    "\n",
    "# evaluate performance\n",
    "ft.evaluate_performance(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get AUC_ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
