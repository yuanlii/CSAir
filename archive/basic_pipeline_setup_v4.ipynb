{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "README: The scripts below used to build the basic pipeline of classification modeling. <br>\n",
    " -- updated: scripts add countvectorizer() and tf-idf transformer to build word vectors; also remove non-Chinese characters, stopwords, digits, and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.chdir('/Users/liyuan/desktop/CSAir/codes')\n",
    "\n",
    "from tokenization import Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer \n",
    "\n",
    "class PrepareData():\n",
    "    def __init__(self):\n",
    "        self.data = pd.DataFrame()\n",
    "        self.train = pd.DataFrame()\n",
    "        self.test = pd.DataFrame()\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        return self.data\n",
    "    \n",
    "    def split_data(self):\n",
    "        self.train, self.test = train_test_split(self.data, test_size = 0.33, random_state=42)\n",
    "        print('training data has %d examples' %len(self.train))\n",
    "        print('test data has %d examples' %len(self.test))\n",
    "        return self.train, self.test\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        '''use countvectorizer and tf-idf transformer to get valid one-hot encoding for reviews'''\n",
    "        # use countVectorizer for one-hot encoding\n",
    "        count_v0= CountVectorizer();  \n",
    "        counts_all = count_v0.fit_transform(self.data['review_tokens'])\n",
    "        count_v1= CountVectorizer(vocabulary=count_v0.vocabulary_)  \n",
    "        counts_train = count_v1.fit_transform(self.train.review_tokens)\n",
    "        print (\"the shape of train word vectors is \"+repr(counts_train.shape))\n",
    "\n",
    "        count_v2 = CountVectorizer(vocabulary=count_v0.vocabulary_)\n",
    "        counts_test = count_v2.fit_transform(self.test.review_tokens)\n",
    "        print (\"the shape of test word vectors is \"+repr(counts_test.shape))\n",
    "\n",
    "        # implement tf-idf\n",
    "        tfidftransformer = TfidfTransformer()\n",
    "        train_data = tfidftransformer.fit(counts_train).transform(counts_train)\n",
    "        test_data = tfidftransformer.fit(counts_test).transform(counts_test)\n",
    "        \n",
    "        X_train = train_data\n",
    "        y_train = self.train.label_encoded\n",
    "        X_test = test_data\n",
    "        y_test = self.test.label_encoded\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    def get_precision(self,y_pred, y_test):\n",
    "        '''this function returns a precision score for the model'''\n",
    "        num = 0\n",
    "        y_pred = y_pred.tolist()\n",
    "        for i,pred in enumerate(y_pred):\n",
    "            if int(pred) == int(y_test.values[i]):\n",
    "                num += 1\n",
    "        precision = float(num) / len(y_pred)\n",
    "        print('precision: '+'{:.2f}'.format(precision))\n",
    "        return precision\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data has 1139 examples\n",
      "test data has 561 examples\n",
      "the shape of train word vectors is (1139, 4246)\n",
      "the shape of test word vectors is (561, 4246)\n"
     ]
    }
   ],
   "source": [
    "data_p = PrepareData()\n",
    "data_p.load_data('./res/all_labeled_data.csv')\n",
    "train, test = data_p.split_data()\n",
    "X_train, y_train, X_test, y_test = data_p.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class Modeling():\n",
    "    def __init__(self,X_train, y_train, X_test, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def get_precision(self,y_pred):\n",
    "        '''this function returns a precision score for the model'''\n",
    "        num = 0\n",
    "        y_pred = y_pred.tolist()\n",
    "        for i,pred in enumerate(y_pred):\n",
    "            if int(pred) == int(self.y_test.values[i]):\n",
    "                num += 1\n",
    "        precision = float(num) / len(y_pred)\n",
    "        #print('precision: '+'{:.2f}'.format(precision))\n",
    "        return precision\n",
    "    \n",
    "    def get_clf_result(self,model):\n",
    "        clf = model   \n",
    "        clf.fit(self.X_train, self.y_train);\n",
    "        y_pred = clf.predict(self.X_test)\n",
    "        result = classification_report(self.y_test, y_pred)\n",
    "        print('performance of classifier:')\n",
    "        print(result)\n",
    "        \n",
    "        # get average accuracy score across classes\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "        print('accuracy scores:',scores)\n",
    "        print('average accuracy score:'+ '{:.2f}'.format(np.average(scores)))\n",
    "\n",
    "        # use precision as evaluation metrics\n",
    "        precision = self.get_precision(y_pred)\n",
    "        return precision\n",
    "    \n",
    "    def grid_search(self,model, parameters):\n",
    "        # use \"f1_weightes\" as evaluation metrics\n",
    "        clf = GridSearchCV(model, parameters, cv=5, scoring = 'f1_weighted')\n",
    "        clf.fit(self.X_train, self.y_train)\n",
    "        print('best parameters of clf are: ')\n",
    "        return clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance of classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.48      0.48        50\n",
      "           1       0.51      0.52      0.52       134\n",
      "           2       0.52      0.57      0.54        44\n",
      "           3       0.65      0.60      0.62        67\n",
      "           4       0.34      0.41      0.37        46\n",
      "           5       0.64      0.64      0.64        94\n",
      "           6       0.33      0.33      0.33        21\n",
      "           7       0.29      0.27      0.28        15\n",
      "           8       0.28      0.23      0.25        22\n",
      "           9       0.56      0.51      0.54        68\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       561\n",
      "   macro avg       0.46      0.46      0.46       561\n",
      "weighted avg       0.52      0.52      0.52       561\n",
      "\n",
      "accuracy scores: [0.53246753 0.50649351 0.48908297 0.49115044 0.47747748]\n",
      "average accuracy score:0.50\n",
      "precision of clf: 0.515\n",
      "=========================================================\n",
      "performance of classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.52      0.58        50\n",
      "           1       0.65      0.60      0.63       134\n",
      "           2       0.63      0.50      0.56        44\n",
      "           3       0.65      0.55      0.60        67\n",
      "           4       0.39      0.24      0.30        46\n",
      "           5       0.42      0.85      0.56        94\n",
      "           6       0.64      0.43      0.51        21\n",
      "           7       1.00      0.20      0.33        15\n",
      "           8       0.00      0.00      0.00        22\n",
      "           9       0.58      0.59      0.58        68\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       561\n",
      "   macro avg       0.56      0.45      0.47       561\n",
      "weighted avg       0.56      0.55      0.53       561\n",
      "\n",
      "accuracy scores: [0.54978355 0.6017316  0.5371179  0.55752212 0.53603604]\n",
      "average accuracy score:0.56\n",
      "precision of clf: 0.551\n",
      "=========================================================\n",
      "performance of classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.54      0.56        50\n",
      "           1       0.56      0.63      0.59       134\n",
      "           2       0.62      0.48      0.54        44\n",
      "           3       0.74      0.60      0.66        67\n",
      "           4       0.37      0.35      0.36        46\n",
      "           5       0.56      0.79      0.65        94\n",
      "           6       0.37      0.33      0.35        21\n",
      "           7       0.75      0.20      0.32        15\n",
      "           8       0.00      0.00      0.00        22\n",
      "           9       0.54      0.60      0.57        68\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       561\n",
      "   macro avg       0.51      0.45      0.46       561\n",
      "weighted avg       0.55      0.56      0.54       561\n",
      "\n",
      "accuracy scores: [0.55844156 0.58441558 0.51965066 0.57079646 0.54954955]\n",
      "average accuracy score:0.56\n",
      "precision of clf: 0.558\n",
      "=========================================================\n",
      "try GridSearch ...\n",
      "best parameters of clf are: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Modeling(X_train, y_train, X_test, y_test)\n",
    "print('precision of clf: {:.3f}'.format(m.get_clf_result(MultinomialNB(alpha = 0.01))))\n",
    "print('=========================================================')\n",
    "print('precision of clf: {:.3f}'.format(m.get_clf_result(LogisticRegression(C=1, penalty='l1'))))\n",
    "print('=========================================================')\n",
    "print('precision of clf: {:.3f}'.format(m.get_clf_result(SVC(kernel='linear'))))\n",
    "print('=========================================================')\n",
    "print('try GridSearch ...')\n",
    "model = LogisticRegression()\n",
    "parameters = {'penalty':('l1', 'l2'), 'C':[0.1, 1, 10]}\n",
    "m.grid_search(model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
