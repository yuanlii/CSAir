{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "README: The scripts below used to build the basic pipeline of classification modeling. <br>\n",
    " -- updated: scripts add countvectorizer() and tf-idf transformer to build word vectors; also remove non-Chinese characters, stopwords, digits, and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.chdir('/Users/liyuan/desktop/CSAir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenization():\n",
    "#     def __init__(self, input_data, output_name, stopwords):\n",
    "    def __init__(self, input_path, output_name, stopwords):\n",
    "        #self.input = input_data[:]\n",
    "        self.input_path = input_path\n",
    "        self.input = []\n",
    "        self.output_name = str(output_name)\n",
    "        self.sentences = []\n",
    "        self.tfidf_score = []\n",
    "        self.stopwords = stopwords\n",
    "    \n",
    "    def load_input_data(self):\n",
    "        with open(self.input_path, 'r', encoding='utf-8') as input_file:\n",
    "            self.input +=  input_file.readlines()\n",
    "            self.input = self.input[:]\n",
    "        return self.input\n",
    "        \n",
    "    def get_tokenized_sents(self):        \n",
    "        for sent in self.input:\n",
    "            tokenized_sent = ' '.join(word for word in jieba.cut(sent.strip()) if word not in self.stopwords)\n",
    "            # remove digits\n",
    "            tokenized_sent = re.sub(r'\\d+','',tokenized_sent)\n",
    "            # remove punctuation\n",
    "            tokenized_sent = re.sub(r'[^\\w\\s]','', tokenized_sent)\n",
    "            # remove non-chinese characters\n",
    "            # match all Chinese words\n",
    "            re_words = re.compile(u\"[\\u4e00-\\u9fa5]+\")\n",
    "            res = re.findall(re_words, tokenized_sent)\n",
    "            if res:\n",
    "                valid_tokenized_sent = ' '.join([r for r in res])\n",
    "            self.sentences.append(valid_tokenized_sent)\n",
    "        \n",
    "        with open(self.output_name + '.txt','w',newline='') as output_file:\n",
    "            for line in self.sentences:\n",
    "                output_file.write(line + '\\n')  \n",
    " \n",
    "        return self.sentences\n",
    "    \n",
    "    def get_topN_tf_idf(self, content, topK=20):\n",
    "        tags = jieba.analyse.extract_tags(content, topK)\n",
    "        return \" \".join(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [line.strip() for line in open('Source_Data/stopwords.txt', 'r', encoding='utf-8').readlines()]   \n",
    "categories = ['中转','出发','到达','售后','性能','机上','行程管理','计划','设计','预订']\n",
    "for cat in categories:\n",
    "    # tokenize data for each class\n",
    "    tok = Tokenization('./Source_Data/CSV_files/'+ cat +'.csv','./Output_Data/output_v2/'+cat,stopwords)\n",
    "    input_data = tok.load_input_data()\n",
    "    sampled_reviews_tokenized = tok.get_tokenized_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the whole dataset include 1700 reviews\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>优惠券</td>\n",
       "      <td>预订</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>优惠券</td>\n",
       "      <td>预订</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>买票 提醒 优惠券</td>\n",
       "      <td>预订</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>专享 优惠 入口 机票 预订 选择 添加 请问 情况</td>\n",
       "      <td>预订</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>参加 月份 会员 日 活动 抽 中国 往返机票 免票 客服 答复 说 月 日 发放 优惠券 ...</td>\n",
       "      <td>预订</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>预定 今日 机票 仓 符合 优惠券 条件</td>\n",
       "      <td>预订</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>上次 买 机票 买不到 优惠 感觉 很坑 浪费 时间</td>\n",
       "      <td>预订</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>注册 会员 会员 专享 购票 优惠活动 朋友 操作 试过 安卓 手机 苹果 手机 不行 客服...</td>\n",
       "      <td>预订</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>连云港 飞 广州 会员 日 一月份 日期 优惠 力度 一点 乘客 谢谢</td>\n",
       "      <td>预订</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>公务舱 优惠 研究 明白 南航 查某 航班 价格 携程 查 南航 搞笑</td>\n",
       "      <td>预订</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       review_tokens label  label_encoded\n",
       "0                                                优惠券    预订              9\n",
       "1                                                优惠券    预订              9\n",
       "2                                          买票 提醒 优惠券    预订              9\n",
       "3                         专享 优惠 入口 机票 预订 选择 添加 请问 情况    预订              9\n",
       "4  参加 月份 会员 日 活动 抽 中国 往返机票 免票 客服 答复 说 月 日 发放 优惠券 ...    预订              9\n",
       "5                               预定 今日 机票 仓 符合 优惠券 条件    预订              9\n",
       "6                         上次 买 机票 买不到 优惠 感觉 很坑 浪费 时间    预订              9\n",
       "7  注册 会员 会员 专享 购票 优惠活动 朋友 操作 试过 安卓 手机 苹果 手机 不行 客服...    预订              9\n",
       "8                连云港 飞 广州 会员 日 一月份 日期 优惠 力度 一点 乘客 谢谢    预订              9\n",
       "9                公务舱 优惠 研究 明白 南航 查某 航班 价格 携程 查 南航 搞笑    预订              9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine \"output\" data files and add encoded label as new column\n",
    "files= glob.glob('./Output_Data/output_v2/*.txt')\n",
    "df_lst = []\n",
    "for f in files:\n",
    "    label = f.split('/')[-1][:2]\n",
    "    df = pd.read_csv(f,header=None)\n",
    "    df['label'] = label\n",
    "    df_lst.append(df)\n",
    "labeled_df = pd.concat(df_lst)\n",
    "print('the whole dataset include %d reviews'%len(labeled_df))\n",
    "labeled_df = labeled_df.rename(columns = {0:'review_tokens'})\n",
    "\n",
    "# encode text label into numbers\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "targets = le.fit_transform(labeled_df.label)\n",
    "labeled_df['label_encoded'] = targets\n",
    "labeled_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'预订': 222, '出发': 367, '设计': 50, '性能': 153, '到达': 151, '行程': 61, '机上': 308, '计划': 39, '中转': 149, '售后': 200}\n"
     ]
    }
   ],
   "source": [
    "# get the data size for each label\n",
    "# may have duplication between classes\n",
    "labels = labeled_df.label.unique().tolist()\n",
    "label_size = {}\n",
    "for label in labels:\n",
    "    label_size[label] = len(labeled_df[labeled_df.label == label])\n",
    "print(label_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data has 1139 examples\n",
      "test data has 561 examples\n"
     ]
    }
   ],
   "source": [
    "### train test split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(labeled_df, test_size=0.33, random_state=42)\n",
    "print('training data has %d examples' %len(train))\n",
    "print('test data has %d examples' %len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of train word vectors is (1139, 4246)\n",
      "the shape of test word vectors is (561, 4246)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer \n",
    "\n",
    "def get_word_vectors(data,train,test):\n",
    "    '''use countvectorizer and tf-idf transformer to get valid one-hot encoding for reviews'''\n",
    "    # use countVectorizer for one-hot encoding\n",
    "    count_v0= CountVectorizer();  \n",
    "    counts_all = count_v0.fit_transform(data['review_tokens'])\n",
    "    count_v1= CountVectorizer(vocabulary=count_v0.vocabulary_)  \n",
    "    counts_train = count_v1.fit_transform(train.review_tokens)\n",
    "    print (\"the shape of train word vectors is \"+repr(counts_train.shape))\n",
    "\n",
    "    count_v2 = CountVectorizer(vocabulary=count_v0.vocabulary_)\n",
    "    counts_test = count_v2.fit_transform(test.review_tokens)\n",
    "    print (\"the shape of test word vectors is \"+repr(counts_test.shape))\n",
    "\n",
    "    # implement tf-idf\n",
    "    tfidftransformer = TfidfTransformer()\n",
    "    train_data = tfidftransformer.fit(counts_train).transform(counts_train)\n",
    "    test_data = tfidftransformer.fit(counts_test).transform(counts_test)\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = get_word_vectors(labeled_df,train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(y_pred, y_test):\n",
    "    '''this function returns a precision score for the model'''\n",
    "    num = 0\n",
    "    y_pred = y_pred.tolist()\n",
    "    for i,pred in enumerate(y_pred):\n",
    "        if int(pred) == int(y_test.values[i]):\n",
    "            num += 1\n",
    "    precision = float(num) / len(y_pred)\n",
    "    print('precision: '+'{:.2f}'.format(precision))\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data\n",
    "y_train = train.label_encoded\n",
    "X_test = test_data\n",
    "y_test = test.label_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance of classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.48      0.48        50\n",
      "           1       0.51      0.52      0.52       134\n",
      "           2       0.52      0.57      0.54        44\n",
      "           3       0.65      0.60      0.62        67\n",
      "           4       0.34      0.41      0.37        46\n",
      "           5       0.64      0.64      0.64        94\n",
      "           6       0.33      0.33      0.33        21\n",
      "           7       0.29      0.27      0.28        15\n",
      "           8       0.28      0.23      0.25        22\n",
      "           9       0.56      0.51      0.54        68\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       561\n",
      "   macro avg       0.46      0.46      0.46       561\n",
      "weighted avg       0.52      0.52      0.52       561\n",
      "\n",
      "scores: [0.53246753 0.50649351 0.48908297 0.49115044 0.47747748]\n",
      "average accuracy score:0.50\n",
      "precision: 0.52\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# use Naive bayes to build classifier\n",
    "clf = MultinomialNB(alpha = 0.01)   \n",
    "clf.fit(X_train, y_train);\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# get classification report\n",
    "result = classification_report(y_test, y_pred)\n",
    "print('performance of classifier:')\n",
    "print(result)\n",
    "\n",
    "# get average accuracy score across classes\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "print('scores:',scores)\n",
    "print('average accuracy score:'+ '{:.2f}'.format(np.average(scores)))\n",
    "\n",
    "# use precision as evaluation metrics\n",
    "precision = get_precision(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'penalty': 'l1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression as classifier and use grid search to find best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "parameters = {'penalty':('l1', 'l2'), 'C':[0.1, 1, 10]}\n",
    "model = LogisticRegression()\n",
    "\n",
    "# use \"f1_weightes\" as evaluation metrics (see below more explanation)\n",
    "clf = GridSearchCV(model, parameters, cv=5, scoring = 'f1_weighted')\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance of classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.52      0.58        50\n",
      "           1       0.65      0.60      0.63       134\n",
      "           2       0.63      0.50      0.56        44\n",
      "           3       0.65      0.55      0.60        67\n",
      "           4       0.39      0.24      0.30        46\n",
      "           5       0.42      0.85      0.56        94\n",
      "           6       0.64      0.43      0.51        21\n",
      "           7       1.00      0.20      0.33        15\n",
      "           8       0.00      0.00      0.00        22\n",
      "           9       0.58      0.59      0.58        68\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       561\n",
      "   macro avg       0.56      0.45      0.47       561\n",
      "weighted avg       0.56      0.55      0.53       561\n",
      "\n",
      "precision: 0.55\n"
     ]
    }
   ],
   "source": [
    "# use the best parameter for logistic regression\n",
    "model = LogisticRegression(C=1, penalty='l1')\n",
    "y_pred = clf.predict(X_test)\n",
    "result = classification_report(y_test, y_pred)\n",
    "print('performance of classifier:')\n",
    "print(result)\n",
    "\n",
    "# use precision as evaluation metrics\n",
    "precision = get_precision(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance of classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.52      0.58        50\n",
      "           1       0.65      0.60      0.63       134\n",
      "           2       0.63      0.50      0.56        44\n",
      "           3       0.65      0.55      0.60        67\n",
      "           4       0.39      0.24      0.30        46\n",
      "           5       0.42      0.85      0.56        94\n",
      "           6       0.64      0.43      0.51        21\n",
      "           7       1.00      0.20      0.33        15\n",
      "           8       0.00      0.00      0.00        22\n",
      "           9       0.58      0.59      0.58        68\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       561\n",
      "   macro avg       0.56      0.45      0.47       561\n",
      "weighted avg       0.56      0.55      0.53       561\n",
      "\n",
      "precision: 0.55\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# implement linearSVC\n",
    "model = SVC(kernel='linear')\n",
    "y_pred = clf.predict(X_test)\n",
    "result = classification_report(y_test, y_pred)\n",
    "print('performance of classifier:')\n",
    "print(result)\n",
    "\n",
    "# use precision as evaluation metrics\n",
    "precision = get_precision(y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
