{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Import data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>机场升级，服务降级。今天到T2找了半天中转休息室，走到三楼服务员说我的计分不够，只能提供金银...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>今天来广州T2中转，服务员说现在不提供中转休息。我6个多小时。严重抗议！我们的权益被南航剥夺</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>页面字体可以稍微大一些。比如中转部分眼神儿不好，根本看不清楚。还有各项功能，也可以稍微大一些...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>我今天乘坐南航从上海到旧金山的飞机CZ3544，需要在武汉中转。票号7821127333。上...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我从始至终选乘南方航空的飞机无论国内行程国外行程 无论票价高低 但是在累积里程的时候我发现 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  机场升级，服务降级。今天到T2找了半天中转休息室，走到三楼服务员说我的计分不够，只能提供金银...\n",
       "1     今天来广州T2中转，服务员说现在不提供中转休息。我6个多小时。严重抗议！我们的权益被南航剥夺\n",
       "2  页面字体可以稍微大一些。比如中转部分眼神儿不好，根本看不清楚。还有各项功能，也可以稍微大一些...\n",
       "3  我今天乘坐南航从上海到旧金山的飞机CZ3544，需要在武汉中转。票号7821127333。上...\n",
       "4  我从始至终选乘南方航空的飞机无论国内行程国外行程 无论票价高低 但是在累积里程的时候我发现 ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz = pd.read_csv('../CSV_files/中转.csv',header = None)\n",
    "zz = zz.rename(columns = {0:'reviews'})\n",
    "zz.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_0 = zz.reviews.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/_d/b3chzbkx5vgg1wtjm942qj4m0000gn/T/jieba.cache\n",
      "Loading model cost 1.058 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 机场/ 升级/ ，/ 服务/ 降级/ 。/ 今天/ 到/ T2/ 找/ 了/ 半天/ 中转/ 休息室/ ，/ 走到/ 三楼/ 服务员/ 说/ 我/ 的/ 计分/ 不够/ ，/ 只能/ 提供/ 金银/ 卡/ 会员/ 休息/ 。/ 然后/ 到/ 中转/ 服务/ 询问/ ，/ 回答/ 我/ 南航/ 转/ 到/ 丅/ 2/ 后/ 暂时/ 没有/ 这个/ 服务/ 。/ 想/ 问/ 一下/ 你们/ 可以/ 随便/ 取消/ 这项/ 服务/ 吗/ ？/ 是不是/ 机场/ 再/ 升级/ 一次/ ，/ 下次/ 就/ 取消/ 所有/ 的/ 座位/ 。\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse\n",
    "\n",
    "seg_list = jieba.cut(review_0, cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('机场', 2.014140778422857),\n",
       " ('计分', 1.5508793163285712),\n",
       " ('降级', 1.3671349016699998),\n",
       " ('服务员', 1.2728921521714285),\n",
       " ('座位', 1.1338711869085716),\n",
       " ('会员', 1.0640898139342858)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 关键词提取\n",
    "keywords_0 = jieba.analyse.extract_tags(review_0, topK=20, withWeight=True, allowPOS=('n','nr','ns'))\n",
    "keywords_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除stop words \n",
    "# 直接采用open file as read 模式；区别于之前使用pandas来process.\n",
    "\n",
    "# 创建停用词list  \n",
    "def stopwordslist(filepath):  \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]  \n",
    "    return stopwords  \n",
    "\n",
    "# 对句子进行分词  \n",
    "def seg_sentence(sentence):  \n",
    "    sentence_seged = jieba.cut(sentence.strip())  \n",
    "    stopwords = stopwordslist('../stopwords.txt')  # 这里加载停用词的路径  \n",
    "    outstr = ''  \n",
    "    for word in sentence_seged:  \n",
    "        if word not in stopwords:  \n",
    "            if word != '\\t':  \n",
    "                outstr += word  \n",
    "                outstr += \" \"  \n",
    "    return outstr \n",
    "\n",
    "# handle tf-idf (Q: tf-idf based on single doc. rather than the whole corpus? 有意义吗？)\n",
    "def get_topN_tf_idf(content,topK=20):\n",
    "    tags = jieba.analyse.extract_tags(content, topK)\n",
    "    print(\" \".join(tags))\n",
    "    \n",
    "inputs = open('../CSV_files/中转.csv', 'r', encoding='utf-8')  \n",
    "outputs = open('../CSV_files/中转output.txt', 'w')  \n",
    "for line in inputs:  \n",
    "    line_seg = seg_sentence(line)  # 这里的返回值是字符串  \n",
    "#     line_seg = get_topN_tf_idf（line_seg,topK=20)   # 移除低频词\n",
    "    outputs.write(line_seg + '\\n')  \n",
    "outputs.close()  \n",
    "inputs.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_file(inputs_path,outputs_path):\n",
    "    '''\n",
    "    This function inputs the source data files, \n",
    "    and return the 分词 file as ouput.    \n",
    "    '''\n",
    "    \n",
    "    def stopwordslist(filepath):  \n",
    "        stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]  \n",
    "        return stopwords  \n",
    "\n",
    "    # 对句子进行分词  \n",
    "    def seg_sentence(sentence):  \n",
    "        sentence_seged = jieba.cut(sentence.strip())  \n",
    "        stopwords = stopwordslist('./stopwords.txt')  # 这里加载停用词的路径  \n",
    "        outstr = ''  \n",
    "        for word in sentence_seged:  \n",
    "            if word not in stopwords:  \n",
    "                if word != '\\t':  \n",
    "                    outstr += word  \n",
    "                    outstr += \" \"  \n",
    "        return outstr \n",
    "\n",
    "    # handle tf-idf (Q: tf-idf based on single doc. rather than the whole corpus? 有意义吗？)\n",
    "    def get_topN_tf_idf(content,topK=20):\n",
    "        tags = jieba.analyse.extract_tags(content, topK)\n",
    "        print(\" \".join(tags))\n",
    "    \n",
    "    inputs = open(inputs_path, 'r', encoding='utf-8')  \n",
    "    outputs = open(outputs_path, 'w')  \n",
    "    for line in inputs:  \n",
    "        line_seg = seg_sentence(line)  # 这里的返回值是字符串  \n",
    "#         line_seg = get_topN_tf_idf（line_seg,topK=20)  # 移除低频词\n",
    "        outputs.write(line_seg + '\\n')  \n",
    "    outputs.close()  \n",
    "    inputs.close() \n",
    "\n",
    "\n",
    "categories = ['中转','出发','到达','售后','性能','机上','行程管理','计划','设计','预订']\n",
    "    \n",
    "for cat in categories:\n",
    "    input_path = '../CSV_files/' + cat + '.csv'\n",
    "    output_path = '../output_data/' + cat + 'output.txt'\n",
    "    get_output_file(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: word2vec (法1)\n",
    "TODO: doc2vec  (法2)\n",
    "TODO: LDA   (法3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the whole dataset include 1623 reviews\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11 月 15 日 提前 预订 2018 年 11 月 27 日 长沙 飞往 沈阳 cz3...</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>航班 延误 登机口 升舱 活动 以原 航班 起飞时间 为准 办理 理解</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>重庆 乌鲁木齐 南航 航班 天气 原因 延误 和田 乘坐 天津 航班</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>沿途 停靠 理解 延误 小时</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>飞机 无故 延误 小时 脸</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>延误 五个 小时 算上 值机 时间 机场 八个 小时 早上 晚上 解释 解决方案 机长 人影...</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cz3842 航班 延误 投诉无门 十点 五十 起飞 下午 三点 弄 飞机 两个 小时 告知...</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>南航 航班 延误 发 短信 太 严谨 回复 改 航班 用户名 密码 我要 变更 航班 做 延...</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>行李 延误   重大损失</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>确认 航班 延误   订 票   显示 确认</td>\n",
       "      <td>出发</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       review_tokens label\n",
       "0   11 月 15 日 提前 预订 2018 年 11 月 27 日 长沙 飞往 沈阳 cz3...    出发\n",
       "1               航班 延误 登机口 升舱 活动 以原 航班 起飞时间 为准 办理 理解     出发\n",
       "2                重庆 乌鲁木齐 南航 航班 天气 原因 延误 和田 乘坐 天津 航班     出发\n",
       "3                                    沿途 停靠 理解 延误 小时     出发\n",
       "4                                     飞机 无故 延误 小时 脸     出发\n",
       "5  延误 五个 小时 算上 值机 时间 机场 八个 小时 早上 晚上 解释 解决方案 机长 人影...    出发\n",
       "6  cz3842 航班 延误 投诉无门 十点 五十 起飞 下午 三点 弄 飞机 两个 小时 告知...    出发\n",
       "7  南航 航班 延误 发 短信 太 严谨 回复 改 航班 用户名 密码 我要 变更 航班 做 延...    出发\n",
       "8                                      行李 延误   重大损失     出发\n",
       "9                            确认 航班 延误   订 票   显示 确认     出发"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: get all labeled data\n",
    "import glob\n",
    "\n",
    "'''\n",
    "combine dataset (multiple categories) into one single category;\n",
    "add a column called 'label'\n",
    "'''\n",
    "\n",
    "files= glob.glob('../output_data/*.txt')\n",
    "\n",
    "df_lst = []\n",
    "for f in files:\n",
    "    label = f.split('/')[-1][:2]\n",
    "    df = pd.read_csv(f,header=None)\n",
    "    df['label'] = label\n",
    "    df_lst.append(df)\n",
    "\n",
    "all_df = pd.concat(df_lst)\n",
    "print('the whole dataset include %d reviews'%len(all_df))\n",
    "all_df = all_df.rename(columns = {0:'review_tokens'})\n",
    "all_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Build classifier pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier reaches accuracy score: 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def build_clf_pipeline(data, pipeline_lst):\n",
    "    '''\n",
    "    This function takes in the whole dataset, and the pipeline set up as a list.\n",
    "    Output the accuracy of the classifier pipeline.\n",
    "    '''\n",
    "    # vectorize data\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(all_df.review_tokens)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    \n",
    "    # build clf\n",
    "    text_clf_svm = Pipeline(pipeline_lst)\n",
    "    text_clf_svm.fit(all_df.review_tokens.values, all_df.label.values)\n",
    "    \n",
    "    # get accuracy score\n",
    "    predicted_labels = text_clf_svm.predict(all_df.review_tokens.values)\n",
    "    accuracy = accuracy_score(all_df.label.values, predicted_labels)\n",
    "    print('Classifier reaches accuracy score: {:.2f}'.format(accuracy))\n",
    "\n",
    "    \n",
    "# SVM clf\n",
    "pipeline_lst = [\n",
    "    ('vect',CountVectorizer()),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('clf-svm',SGDClassifier(penalty='l2',alpha =1e-3, n_iter = 5, random_state = 0))]\n",
    "\n",
    "SVM_accuracy = build_clf_pipeline(all_df,pipeline_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier reaches accuracy score: 0.83\n"
     ]
    }
   ],
   "source": [
    "# linearSVC\n",
    "from sklearn.svm import SVC\n",
    "pipeline_lst = [\n",
    "    ('vect',CountVectorizer()),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('clf-linearSVC',SVC(kernel='linear'))]\n",
    "\n",
    "linearSVC_accuracy = build_clf_pipeline(all_df,pipeline_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier reaches accuracy score: 0.65\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline_lst = [\n",
    "    ('vect',CountVectorizer()),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('clf',MultinomialNB())]\n",
    "\n",
    "NB_accuracy = build_clf_pipeline(all_df,pipeline_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier reaches accuracy score: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/nlp/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/envs/nlp/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline_lst = [\n",
    "    ('vect',CountVectorizer()),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('clf',LogisticRegression())]\n",
    "\n",
    "LR_accuracy = build_clf_pipeline(all_df,pipeline_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> TODO: 使用neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用neural network可以免去特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN (法4) -> textCNN\n",
    "# RNN (法5) -> textRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
